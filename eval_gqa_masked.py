#!/usr/bin/env python3
"""
GQAè¯„ä¼°è„šæœ¬ï¼Œæ”¯æŒè§†è§‰token maskæ¶ˆèå®éªŒ
æ”¯æŒæœ¬åœ°å’Œè¿œç¨‹GQAæ•°æ®é›†
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np
import random

# å›ºå®šéšæœºç§å­
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

# è®¾ç½®éšæœºç§å­
set_seed(42)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
# ç›´æ¥ä»utils.pyæ–‡ä»¶å¯¼å…¥
import importlib.util
utils_path = Path(__file__).parent / "llava" / "utils.py"
spec = importlib.util.spec_from_file_location("llava_utils", utils_path)
llava_utils = importlib.util.module_from_spec(spec)
spec.loader.exec_module(llava_utils)
disable_torch_init = llava_utils.disable_torch_init
from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path
from llava.utils.attention_visualizer import AttentionVisualizer
from llava.data.gqa_loader import GQALoader

from PIL import Image
import transformers


def setup_args():
    parser = argparse.ArgumentParser(description="GQAè¯„ä¼°è„šæœ¬ - æ”¯æŒè§†è§‰token mask")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_path", type=str, default="liuhaotian/llava-v1.5-7b")
    parser.add_argument("--model_base", type=str, default=None)
    parser.add_argument("--model_name", type=str, default=None)
    parser.add_argument("--device", type=str, default="cuda:1", help="è®¾å¤‡ç±»å‹ï¼Œæ”¯æŒæŒ‡å®šGPUå¦‚cuda:1")
    parser.add_argument("--load_in_8bit", action="store_true", help="ä½¿ç”¨8bité‡åŒ–åŠ è½½æ¨¡å‹")
    parser.add_argument("--load_in_4bit", action="store_true", help="ä½¿ç”¨4bité‡åŒ–åŠ è½½æ¨¡å‹")
    parser.add_argument("--torch_dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"], help="æ¨¡å‹æ•°æ®ç±»å‹")
    parser.add_argument("--clip_path", type=str, default="/home/czj/llava15_test/clip-vit-large-patch14-336", help="CLIPè§†è§‰ç¼–ç å™¨æœ¬åœ°è·¯å¾„")
    parser.add_argument("--vision_tower", type=str, default="openai/clip-vit-large-patch14-336", help="è§†è§‰ç¼–ç å™¨åç§°ï¼Œå¦‚æœæœ¬åœ°ä¸å­˜åœ¨åˆ™ä»ç½‘ä¸Šä¸‹è½½")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--gqa_root", type=str, default="/home/Dataset/Dataset/GQA", help="æœ¬åœ°GQAæ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--use_remote", action="store_true", help="ä½¿ç”¨è¿œç¨‹HuggingFaceæ•°æ®é›†")
    parser.add_argument("--dataset_name", type=str, default="lmms-lab/GQA", help="HuggingFaceæ•°æ®é›†åç§°")
    parser.add_argument("--gqa_split", type=str, default="train_balanced", help="GQAæ•°æ®é›†åˆ†å‰²")
    parser.add_argument("--conv_mode", type=str, default="llava_v1")
    parser.add_argument("--num_samples", type=int, default=None, help="è¯„ä¼°æ ·æœ¬æ•°é‡")
    
    # Maskå‚æ•°
    parser.add_argument("--mask_visual_token", action="store_true", help="æ˜¯å¦å¯ç”¨è§†è§‰token mask")
    parser.add_argument("--mask_ratio", type=float, default=0.1, help="Maskæ¯”ä¾‹ (0.0-1.0)")
    parser.add_argument("--mask_strategy", type=str, default="random", 
                       choices=["random", "attention", "position_center", "position_edge"],
                       help="Maskç­–ç•¥")
    parser.add_argument("--mask_token_value", type=float, default=0.0, help="Maskåçš„tokenå€¼")
    
    # æ³¨æ„åŠ›å¯è§†åŒ–å‚æ•°
    parser.add_argument("--visualize_attention", action="store_true", help="æ˜¯å¦å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡")
    parser.add_argument("--attention_layers", type=str, default="0,6,12,18", help="è¦å¯è§†åŒ–çš„å±‚ç´¢å¼•")
    parser.add_argument("--attention_heads", type=str, default="0,4,8,12", help="è¦å¯è§†åŒ–çš„å¤´ç´¢å¼•")
    parser.add_argument("--save_attention_dir", type=str, default="./attention_visualizations", 
                       help="æ³¨æ„åŠ›å¯è§†åŒ–ä¿å­˜ç›®å½•")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./gqa_results", help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--save_predictions", action="store_true", help="æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ")
    
    return parser.parse_args()


def check_clip_path(clip_path: str, vision_tower: str) -> str:
    """æ£€æŸ¥CLIPè·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›è¿œç¨‹è·¯å¾„"""
    if Path(clip_path).exists():
        print(f"âœ… ä½¿ç”¨æœ¬åœ°CLIPè·¯å¾„: {clip_path}")
        return clip_path
    else:
        print(f"âŒ æœ¬åœ°CLIPè·¯å¾„ä¸å­˜åœ¨: {clip_path}")
        print(f"ğŸ”„ å°†ä½¿ç”¨è¿œç¨‹è·¯å¾„: {vision_tower}")
        return vision_tower

def load_model_and_tokenizer(model_path: str, model_base: Optional[str] = None, device: str = "cuda:1", 
                           load_in_8bit: bool = False, load_in_4bit: bool = False, torch_dtype: str = "float16",
                           clip_path: str = None, vision_tower: str = "openai/clip-vit-large-patch14-336"):
    """åŠ è½½æ¨¡å‹å’Œtokenizer"""
    print(f"åŠ è½½æ¨¡å‹ä» {model_path}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"é‡åŒ–è®¾ç½®: 8bit={load_in_8bit}, 4bit={load_in_4bit}")
    print(f"æ•°æ®ç±»å‹: {torch_dtype}")
    
    # æ£€æŸ¥CLIPè·¯å¾„
    if clip_path and Path(clip_path).exists():
        vision_tower_path = check_clip_path(clip_path, vision_tower)
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©transformersä½¿ç”¨æœ¬åœ°è·¯å¾„
        import os
        os.environ['HF_HUB_OFFLINE'] = '1'  # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
        print(f"ğŸ”§ è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨æœ¬åœ°CLIPè·¯å¾„: {clip_path}")
    else:
        vision_tower_path = vision_tower
    
    # ç¦ç”¨torchåˆå§‹åŒ–ä»¥èŠ‚çœå†…å­˜
    disable_torch_init()
    
    # è®¾ç½®è®¾å¤‡æ˜ å°„
    if device.startswith("cuda:"):
        gpu_id = int(device.split(":")[1])
        device_map = f"cuda:{gpu_id}"
    else:
        device_map = "auto"
    
    # è®¾ç½®æ•°æ®ç±»å‹
    if torch_dtype == "float16":
        dtype = torch.float16
    elif torch_dtype == "bfloat16":
        dtype = torch.bfloat16
    else:
        dtype = torch.float32
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model, image_processor, context_len = load_pretrained_model(
        model_path=model_path,
        model_base=model_base,
        model_name=get_model_name_from_path(model_path),
        load_8bit=load_in_8bit,
        load_4bit=load_in_4bit,
        device_map=device_map,
        torch_dtype=dtype,
        use_masked_model=True  # ä½¿ç”¨maskedæ¨¡å‹
    )
    
    return tokenizer, model, image_processor, context_len


def prepare_question(question: str, conv_mode: str = "llava_v1") -> str:
    """å‡†å¤‡é—®é¢˜æ ¼å¼"""
    conv = conv_templates[conv_mode].copy()
    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + "\n" + question)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    return prompt


def evaluate_model_on_gqa(
    model,
    tokenizer,
    image_processor,
    gqa_loader: GQALoader,
    args
) -> Dict:
    """åœ¨GQAæ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹"""
    
    model.eval()
    results = []
    correct = 0
    total = 0
    
    # åˆå§‹åŒ–æ³¨æ„åŠ›å¯è§†åŒ–å™¨
    attention_visualizer = None
    if args.visualize_attention:
        attention_visualizer = AttentionVisualizer(args.save_attention_dir)
        attention_layers = [int(x) for x in args.attention_layers.split(',')]
        attention_heads = [int(x) for x in args.attention_heads.split(',')]
    
    # åŠ è½½æ•°æ®é›†
    dataset = gqa_loader.load_dataset(split=args.gqa_split, num_samples=args.num_samples)
    
    print(f"å¼€å§‹è¯„ä¼° {len(dataset)} ä¸ªæ ·æœ¬...")
    
    # å¤„ç†æ¯ä¸ªæ ·æœ¬
    for i, sample in enumerate(tqdm(dataset, desc="è¯„ä¼°è¿›åº¦")):
        if args.num_samples and i >= args.num_samples:
            break
            
        try:
            # å¤„ç†æ ·æœ¬ï¼ŒåŠ è½½å›¾åƒ
            processed_sample = gqa_loader.process_sample(sample)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒ
            if processed_sample['image'] is None:
                print(f"æ ·æœ¬ {i} æ²¡æœ‰å›¾åƒï¼Œè·³è¿‡")
                continue
            
            image = processed_sample['image']
            question = processed_sample['question']
            ground_truth = processed_sample['answer']
            
            # å‡†å¤‡é—®é¢˜
            prompt = prepare_question(question, args.conv_mode)
            
            # å¤„ç†å›¾åƒ - ç¡®ä¿å°ºå¯¸ä¸€è‡´
            try:
                # é¦–å…ˆç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # ä½¿ç”¨image_processorç›´æ¥å¤„ç†ï¼Œç¡®ä¿å°ºå¯¸ç»Ÿä¸€
                image_tensor = image_processor(image, return_tensors='pt')['pixel_values']
                image_tensor = image_tensor.to(model.device, dtype=torch.float16)
                
                print(f"  å›¾åƒå¤„ç†æˆåŠŸ: åŸå§‹å°ºå¯¸={image.size}, å¤„ç†åå½¢çŠ¶={image_tensor.shape}")
                
            except Exception as e:
                print(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
                print(f"å›¾åƒå½¢çŠ¶: {image.size}")
                import traceback
                traceback.print_exc()
                continue
            
            # å‡†å¤‡è¾“å…¥
            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                if args.visualize_attention:
                    # éœ€è¦è·å–æ³¨æ„åŠ›æƒé‡
                    outputs = model.generate(
                        input_ids=input_ids,
                        images=image_tensor,
                        do_sample=False,
                        temperature=0,
                        top_p=None,
                        num_beams=1,
                        max_new_tokens=512,
                        output_attentions=True,
                        return_dict_in_generate=True
                    )
                    
                    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
                    if hasattr(outputs, 'attentions') and outputs.attentions:
                        attention_weights = outputs.attentions
                        for layer_idx in attention_layers:
                            if layer_idx < len(attention_weights):
                                for head_idx in attention_heads:
                                    if head_idx < attention_weights[layer_idx].shape[1]:
                                        # å¯è§†åŒ–ç‰¹å®šå±‚çš„æ³¨æ„åŠ›
                                        attention_visualizer.visualize_attention_heatmap(
                                            attention_weights[layer_idx],
                                            layer_idx=layer_idx,
                                            head_idx=head_idx,
                                            title=f"Sample {i} - {question[:50]}..."
                                        )
                else:
                    outputs = model.generate(
                        input_ids,
                        images=image_tensor,
                        do_sample=False,
                        temperature=0,
                        top_p=None,
                        num_beams=1,
                        max_new_tokens=512
                    )
            
            # è§£ç è¾“å‡º
            input_token_len = input_ids.shape[1]
            n_diff_input_output = (input_ids != outputs[0][:len(input_ids[0])]).sum().item()
            if n_diff_input_output > 0:
                print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
            
            outputs = tokenizer.batch_decode(outputs[0][input_token_len:], skip_special_tokens=True)[0]
            outputs = outputs.strip()
            
            # è¯„ä¼°ç­”æ¡ˆ
            predicted_answer = outputs.lower().strip()
            ground_truth_lower = ground_truth.lower().strip()
            
            is_correct = predicted_answer == ground_truth_lower
            if is_correct:
                correct += 1
            total += 1
            
            # ä¿å­˜ç»“æœ
            result = {
                'id': processed_sample['id'],
                'question': question,
                'predicted_answer': predicted_answer,
                'ground_truth': ground_truth,
                'correct': is_correct,
                'imageId': processed_sample['imageId']
            }
            results.append(result)
            
            if i % 100 == 0:
                print(f"è¿›åº¦: {i}/{len(dataset)}, å‡†ç¡®ç‡: {correct/total:.4f}")
                
        except Exception as e:
            print(f"å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
            continue
    
    # è®¡ç®—æœ€ç»ˆç»“æœ
    accuracy = correct / total if total > 0 else 0.0
    
    final_results = {
        'accuracy': accuracy,
        'correct': correct,
        'total': total,
        'results': results,
        'mask_config': {
            'mask_visual_token': args.mask_visual_token,
            'mask_ratio': args.mask_ratio,
            'mask_strategy': args.mask_strategy,
            'mask_token_value': args.mask_token_value
        }
    }
    
    return final_results


def save_results(results: Dict, output_dir: str, args):
    """ä¿å­˜ç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if args.save_predictions:
        results_file = os.path.join(output_dir, "detailed_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ä¿å­˜æ‘˜è¦ç»“æœ
    summary = {
        'accuracy': results['accuracy'],
        'correct': results['correct'],
        'total': results['total'],
        'mask_config': results['mask_config']
    }
    
    summary_file = os.path.join(output_dir, "summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"æ‘˜è¦ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*50)
    print("è¯„ä¼°ç»“æœ:")
    print(f"å‡†ç¡®ç‡: {results['accuracy']:.4f}")
    print(f"æ­£ç¡®: {results['correct']}/{results['total']}")
    print(f"Maské…ç½®: {results['mask_config']}")
    print("="*50)


def main():
    args = setup_args()
    
    print("å¼€å§‹GQAè¯„ä¼°...")
    print(f"å‚æ•°é…ç½®: {vars(args)}")
    
    # åˆ›å»ºGQAåŠ è½½å™¨
    if args.use_remote:
        gqa_loader = GQALoader(dataset_name=args.dataset_name)
        print("ä½¿ç”¨è¿œç¨‹HuggingFaceæ•°æ®é›†")
    else:
        gqa_loader = GQALoader(gqa_root=args.gqa_root)
        print("ä½¿ç”¨æœ¬åœ°GQAæ•°æ®é›†")
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    print("æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        test_dataset = gqa_loader.load_dataset(split=args.gqa_split, num_samples=1)
        dataset_info = gqa_loader.get_dataset_info(test_dataset)
        print(f"æ•°æ®é›†ä¿¡æ¯: {dataset_info}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»“æ„
        if len(test_dataset) > 0:
            sample = test_dataset[0]
            print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®: {list(sample.keys())}")
        
    except Exception as e:
        print(f"æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥GQAæ•°æ®é›†è·¯å¾„å’Œæ ¼å¼")
        return
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model, image_processor, context_len = load_model_and_tokenizer(
        args.model_path, args.model_base, args.device,
        args.load_in_8bit, args.load_in_4bit, args.torch_dtype,
        args.clip_path, args.vision_tower
    )
    
    # è®¾ç½®maskå‚æ•°
    if args.mask_visual_token:
        model.config.mask_visual_token = True
        model.config.mask_ratio = args.mask_ratio
        model.config.mask_strategy = args.mask_strategy
        model.config.mask_token_value = args.mask_token_value
        
        # åˆå§‹åŒ–maskç­–ç•¥å¯¹è±¡
        if args.mask_strategy == 'random':
            from llava.model.llava_arch_masked import RandomMaskStrategy
            model.mask_strategy_obj = RandomMaskStrategy(args.mask_ratio)
        elif args.mask_strategy == 'attention':
            from llava.model.llava_arch_masked import AttentionBasedMaskStrategy
            model.mask_strategy_obj = AttentionBasedMaskStrategy(args.mask_ratio)
        elif args.mask_strategy == 'position_center':
            from llava.model.llava_arch_masked import PositionBasedMaskStrategy
            model.mask_strategy_obj = PositionBasedMaskStrategy(args.mask_ratio, mask_center=True)
        elif args.mask_strategy == 'position_edge':
            from llava.model.llava_arch_masked import PositionBasedMaskStrategy
            model.mask_strategy_obj = PositionBasedMaskStrategy(args.mask_ratio, mask_center=False)
        
        print(f"å¯ç”¨è§†è§‰token mask: æ¯”ä¾‹={args.mask_ratio}, ç­–ç•¥={args.mask_strategy}")
    else:
        model.config.mask_visual_token = False
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model_on_gqa(
        model, tokenizer, image_processor, gqa_loader, args
    )
    
    # ä¿å­˜ç»“æœ
    save_results(results, args.output_dir, args)
    
    print("è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()