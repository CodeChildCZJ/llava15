"""
æ™ºèƒ½ GQA æ•°æ®åŠ è½½å™¨ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰
æ”¯æŒæœ¬åœ° parquet ä¸ HuggingFace è¿œç¨‹æ•°æ®é›†ã€‚
è‡ªåŠ¨è¯†åˆ« train/val/testdev/challenge/submission ç­‰ç»“æ„ã€‚
"""

import os
import pandas as pd
from io import BytesIO
from typing import List, Dict, Optional
from PIL import Image
from datasets import Dataset, load_dataset, load_from_disk
from torch.utils.data import DataLoader
from concurrent.futures import ThreadPoolExecutor, as_completed



class GQALoader:
    """ç»Ÿä¸€çš„ GQA æ•°æ®åŠ è½½å™¨"""

    def __init__(self, data_source: str, num_workers: int = 4, batch_size: int = 8, use_cache: bool = True):
        self.data_source = data_source.strip()
        self.num_workers = num_workers
        self.batch_size = batch_size
        self.use_cache = use_cache

        self.is_local = os.path.exists(self.data_source)
        if self.is_local:
            self.gqa_root = self.data_source
            print(f"ä½¿ç”¨æœ¬åœ° GQA æ•°æ®é›†è·¯å¾„: {self.gqa_root}")
        else:
            self.dataset_name = self.data_source
            print(f"ä½¿ç”¨è¿œç¨‹ GQA æ•°æ®é›†: {self.dataset_name}")

    # ------------------------------------------------------------------
    def _find_best_match_dir(self, split_base: str, kind: str) -> Optional[str]:
        """
        æ™ºèƒ½åŒ¹é…æ–‡ä»¶å¤¹ï¼š
        æ”¯æŒ challenge / submission / testdev / train / val / test
        ä¼˜å…ˆé¡ºåºï¼šbalanced > all
        """
        assert kind in ["instructions", "images"]
        candidates = [
            f"{split_base}_balanced_{kind}",
            f"{split_base}_all_{kind}",
            f"{split_base}_{kind}",
        ]
        for cand in candidates:
            path = os.path.join(self.gqa_root, cand)
            if os.path.exists(path):
                return path
        return None

    # ------------------------------------------------------------------
    def _parallel_read_parquets(self, file_list, key_name="id") -> Dict[str, dict]:
        """å¹¶è¡Œè¯»å–å¤šä¸ª parquet æ–‡ä»¶"""
        image_dict = {}
        if not file_list:
            return image_dict

        with ThreadPoolExecutor(max_workers=min(self.num_workers * 2, len(file_list))) as ex:
            futures = {ex.submit(pd.read_parquet, f): f for f in file_list}
            for fut in as_completed(futures):
                df = fut.result()
                for _, row in df.iterrows():
                    image_dict[row[key_name]] = row["image"]

        print(f"å¹¶è¡ŒåŠ è½½å®Œæˆï¼Œå…± {len(image_dict)} å¼ å›¾åƒ")
        return image_dict

    # ------------------------------------------------------------------
    def load_dataset(self, split: str = "train_balanced", num_samples: Optional[int] = None) -> Dataset:
        """åŠ è½½æœ¬åœ°æˆ–è¿œç¨‹æ•°æ®"""
        if self.is_local:
            return self._load_local_dataset(split, num_samples)
        else:
            return self._load_remote_dataset(split, num_samples)

    # ------------------------------------------------------------------
    def _load_local_dataset(self, split: str, num_samples: Optional[int], use_cache: bool = True) -> Dataset:
        """åŠ è½½æœ¬åœ° GQA parquet æ•°æ®ï¼ˆArrow ç¼“å­˜ç‰ˆï¼‰"""
        print(f"åŠ è½½æœ¬åœ° GQA split: {split}")

        split_base = split.replace("_balanced", "").replace("_all", "")
        instructions_dir = self._find_best_match_dir(split_base, "instructions")
        images_dir = self._find_best_match_dir(split_base, "images")

        if not instructions_dir or not images_dir:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°åŒ¹é…ç›®å½•ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„ã€‚\n"
                f"  æ ¹è·¯å¾„: {self.gqa_root}\n"
                f"  split: {split}\n"
                f"  å¯é€‰: train / val / testdev / challenge / submission / test"
            )

        cache_dir = os.path.join(self.gqa_root, ".gqa_cache")
        os.makedirs(cache_dir, exist_ok=True)
        cache_path = os.path.join(cache_dir, f"{split_base}_arrow")

        # è‹¥å­˜åœ¨ Arrow ç¼“å­˜
        if use_cache and os.path.exists(cache_path):
            print(f"ä» Arrow ç¼“å­˜åŠ è½½: {cache_path}")
            dataset = load_from_disk(cache_path)
            if num_samples:
                dataset = dataset.select(range(min(num_samples, len(dataset))))
            print(f"å·²åŠ è½½ {len(dataset)} æ¡æ ·æœ¬ï¼ˆæ¥è‡ªç¼“å­˜ï¼‰")
            return dataset

        # é¦–æ¬¡åŠ è½½ parquet
        print(f"é¦–æ¬¡åŠ è½½ split={split}ï¼Œè¯»å– parquet æ–‡ä»¶...")
        inst_files = [os.path.join(instructions_dir, f) for f in os.listdir(instructions_dir) if f.endswith(".parquet")]
        img_files = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith(".parquet")]

        if not inst_files or not img_files:
            raise FileNotFoundError("åœ¨æŒ‡å®šç›®å½•ä¸­æœªæ‰¾åˆ° parquet æ–‡ä»¶")

        instruction_df = pd.read_parquet(inst_files[0])
        print(f"æŒ‡ä»¤æ•°æ®: {len(instruction_df)} æ¡")

        image_dict = self._parallel_read_parquets(img_files, "id")
        print(f"å›¾åƒæ•°æ®: {len(image_dict)} å¼ ")

        samples = []
        for _, row in instruction_df.iterrows():
            if num_samples and len(samples) >= num_samples:
                break
            img_id = row.get("imageId")
            if not img_id or img_id not in image_dict:
                continue
            samples.append({
                "id": row.get("id", ""),
                "imageId": img_id,
                "question": row.get("question", ""),
                "answer": row.get("answer", ""),
                "image_data": image_dict[img_id],
            })

        dataset = Dataset.from_list(samples)

        # ç¼“å­˜ä¸º Arrow æ ¼å¼
        dataset.save_to_disk(cache_path)
        print(f"å·²ç¼“å­˜ {len(samples)} æ¡æ ·æœ¬åˆ° {cache_path}ï¼ˆArrow æ ¼å¼ï¼‰")

        print(f"æˆåŠŸåŠ è½½ {len(samples)} æ¡æ ·æœ¬")
        return dataset


    # ------------------------------------------------------------------
    def _load_remote_dataset(self, split: str, num_samples: Optional[int]) -> Dataset:
        """åŠ è½½è¿œç¨‹ HuggingFace æ•°æ®é›†"""
        print(f"åŠ è½½è¿œç¨‹ GQA æ•°æ®é›†: {self.dataset_name}, split={split}")
        ds = load_dataset(self.dataset_name, split=split)
        if num_samples:
            ds = ds.select(range(min(num_samples, len(ds))))
        print(f"è¿œç¨‹æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(ds)} æ¡")
        return ds

    # ------------------------------------------------------------------
    def process_sample(self, sample: Dict) -> Dict:
        """åŠ è½½å•ä¸ªæ ·æœ¬å›¾åƒ"""
        processed = sample.copy()
        image = None
        try:
            if "image" in sample and isinstance(sample["image"], Image.Image):
                image = sample["image"]
            elif "image_data" in sample:
                data = sample["image_data"]
                if isinstance(data, dict) and "bytes" in data:
                    image = Image.open(BytesIO(data["bytes"])).convert("RGB")
            elif "image_path" in sample and os.path.exists(sample["image_path"]):
                image = Image.open(sample["image_path"]).convert("RGB")
        except Exception as e:
            print(f"å›¾åƒåŠ è½½å¤±è´¥: {e}")
        processed["image"] = image
        return processed

     # ------------------------------------------------------------------
    def _decode_image(self, data):
        """å®‰å…¨åœ°ä» bytes è§£ç å›¾åƒ"""
        try:
            if isinstance(data, dict) and "bytes" in data:
                return Image.open(BytesIO(data["bytes"])).convert("RGB")
        except Exception:
            return None
        return None
    
    def collate_fn(self, batch):
        """å¹¶è¡Œ decode å›¾åƒ"""
        def decode_safe(b):
            try:
                if "image_data" in b and b["image_data"] is not None:
                    b["image"] = self._decode_image(b["image_data"])
                else:
                    b["image"] = None
            except Exception:
                b["image"] = None
            return b

        with ThreadPoolExecutor(max_workers=min(self.num_workers, len(batch))) as ex:
            batch = list(ex.map(decode_safe, batch))
        return batch

    # ------------------------------------------------------------------
    def as_dataloader(self, split: str, num_samples: Optional[int] = None):
        dataset = self._load_local_dataset(split, num_samples) if self.is_local else self._load_remote_dataset(split, num_samples)
        dataloader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True,
            shuffle=False,
            collate_fn=self.collate_fn,
        )
        print(f"DataLoader å°±ç»ª: batch_size={self.batch_size}, num_workers={self.num_workers}")
        return dataloader


def test_split(split_name: str, num_samples: int = None):
    print("\n" + "=" * 80)
    print(f"ğŸš€ æµ‹è¯•åŠ è½½ split = '{split_name}'")
    print("=" * 80)

    loader = GQALoader("/home/Dataset/Dataset/GQA")

    try:
        import time
        begin_time = time.time()
        dataset = loader.load_dataset(split=split_name, num_samples=num_samples)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset)} ä¸ªæ ·æœ¬")
        end_time = time.time()
        print(f"åŠ è½½æ—¶é—´: {end_time - begin_time} ç§’")

        if len(dataset) > 0:
            sample = dataset[0]
            print(f"ğŸ“‹ æ ·æœ¬é”®: {list(sample.keys())}")
            print(f"ğŸ§  é—®é¢˜: {sample.get('question')}")
            print(f"ğŸ¯ ç­”æ¡ˆ: {sample.get('answer')}")
            print(f"ğŸ–¼ï¸ å›¾åƒID: {sample.get('imageId')}")
        else:
            print("âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ parquet æ–‡ä»¶å†…å®¹ã€‚")

    except Exception as e:
        print(f"âŒ åŠ è½½ split='{split_name}' å¤±è´¥: {e}")


if __name__ == "__main__":
    # ä¾æ¬¡æµ‹è¯•å„ç§åˆ†å‰²ç±»å‹
    splits_to_test = [
        "train", "val", 
        "testdev",
        "challenge", "submission",
        "test", "train_all", "val_balanced"
    ]



    for s in splits_to_test:
        test_split(s)