#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
evaluate_gqa_lmms.py
è¯„ä¼° LLaVA-1.5ï¼ˆæˆ–å…¶ä»– MLLMï¼‰åœ¨ lmms-lab/GQA ä¸Šçš„è¡¨ç°ã€‚
æ”¯æŒå®½æ¾åŒ¹é… (semantic fuzzy match)ã€‚
"""

import argparse
import json
import re
from tqdm import tqdm
from datasets import load_dataset
from collections import Counter
from PIL import Image
import torch
from eval.pred_gt_match import compute_match


# ==== è¾…åŠ©å‡½æ•° ====

def normalize_text(s: str) -> str:
    """å»é™¤æ ‡ç‚¹ã€å¤§å°å†™ã€å‰åç©ºæ ¼"""
    s = s.lower().strip()
    s = re.sub(r"[^\w\s]", "", s)
    s = re.sub(r"\s+", " ", s)
    return s

def is_match(pred, gold):
    """å®½æ¾åŒ¹é…è§„åˆ™ï¼šå®Œå…¨ç›¸ç­‰ã€æˆ–åŒ…å«å…³ç³»"""
    pred_n = normalize_text(pred)
    gold_n = normalize_text(gold)
    return (pred_n == gold_n) or (gold_n in pred_n) or (pred_n in gold_n)

# ==== ä¸»æµç¨‹ ====

def evaluate_gqa(model, tokenizer, image_processor, model_name="llava-v1.5-7b",
                 split="testdev", output_json="gqa_eval_results.json",
                 max_samples=None, device="cuda"):

    print(f"ğŸ”¹ æ­£åœ¨åŠ è½½æ•°æ®é›† lmms-lab/GQA ({split}) ...")
    dataset = load_dataset("lmms-lab/GQA", "testdev_balanced_instructions", split=split)

    results = []
    correct, total = 0, 0

    for i, sample in enumerate(tqdm(dataset, desc="Evaluating")):
        if max_samples and i >= max_samples:
            break

        image: Image.Image = sample["image"].convert("RGB")
        question = sample["question"]
        gold_answer = sample["answer"]

        # é¢„å¤„ç†å›¾åƒ
        image_tensor = image_processor(image, return_tensors="pt")["pixel_values"].to(device, dtype=torch.float16)

        # æ„é€ è¾“å…¥ï¼ˆLLaVA é£æ ¼ï¼‰
        prompt = f"<image>\n{question}"
        input_ids = tokenizer(
            prompt, return_tensors="pt"
        ).input_ids.to(device)

        with torch.no_grad():
            output_ids = model.generate(
                input_ids=input_ids,
                images=[image_tensor],          # æ³¨æ„è¦ç”¨ list
                do_sample=False,
                temperature=0,
                max_new_tokens=64,
            )

        pred_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

        # è®¡ç®—åŒ¹é…
        total += 1
        if is_match(pred_text, gold_answer):
            correct += 1

        results.append({
            "id": sample["id"],
            "question": question,
            "ground_truth": gold_answer,
            "prediction": pred_text,
            "correct": is_match(pred_text, gold_answer)
        })

        if (i + 1) % 50 == 0:
            acc = correct / total * 100
            print(f"  å·²è¯„ä¼° {i+1} ä¸ªæ ·æœ¬ï¼Œå½“å‰å‡†ç¡®ç‡ï¼š{acc:.2f}%")

    # è®¡ç®—æœ€ç»ˆç»“æœ
    accuracy = correct / total * 100 if total > 0 else 0.0
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼šæ€»æ ·æœ¬æ•° {total}ï¼Œå‡†ç¡®ç‡ = {accuracy:.2f}%")

    # è¾“å‡ºç»“æœ
    output = {
        "model": model_name,
        "split": split,
        "accuracy": accuracy,
        "total": total,
        "correct": correct,
        "results": results
    }
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³ {output_json}")

    # å¯é€‰ï¼šè¾“å‡ºç­”æ¡ˆåˆ†å¸ƒï¼ˆæ–¹ä¾¿åˆ†æï¼‰
    gold_counts = Counter(normalize_text(s["answer"]) for s in dataset)
    pred_counts = Counter(normalize_text(r["prediction"]) for r in results)
    print(f"\nTop-10 goldç­”æ¡ˆåˆ†å¸ƒ: {gold_counts.most_common(10)}")
    print(f"Top-10 é¢„æµ‹ç­”æ¡ˆåˆ†å¸ƒ: {pred_counts.most_common(10)}")

    return output

# ==== å‘½ä»¤è¡Œå…¥å£ ====

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, required=True,
                        help="æ¨¡å‹è·¯å¾„ï¼Œå¦‚ liuhaotian/llava-v1.5-7b")
    parser.add_argument("--split", type=str, default="testdev",
                        help="GQA æ•°æ®åˆ’åˆ†ï¼Œå¦‚ testdev / val")
    parser.add_argument("--max-samples", type=int, default=None,
                        help="ä»…è¯„ä¼°å‰ N æ¡æ ·æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    parser.add_argument("--output", type=str, default="gqa_eval_results.json")
    args = parser.parse_args()

    # ===== åŠ è½½æ¨¡å‹ =====
    from llava.model.builder import load_pretrained_model
    tokenizer, model, image_processor, _ = load_pretrained_model(
        args.model_path, None, "cuda"
    )

    evaluate_gqa(model, tokenizer, image_processor,
                 model_name=args.model_path,
                 split=args.split,
                 output_json=args.output,
                 max_samples=args.max_samples)
