#!/usr/bin/env python3
"""
ç®€åŒ–çš„pipelineæµ‹è¯•ï¼ŒéªŒè¯GQAæ•°æ®åŠ è½½å’ŒmaskåŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from llava.data.gqa_loader import GQALoader
from llava.model.llava_arch_masked import RandomMaskStrategy
import torch
import numpy as np


def test_simple_pipeline():
    """æµ‹è¯•ç®€åŒ–çš„pipeline"""
    print("æµ‹è¯•ç®€åŒ–çš„GQA pipeline...")
    print("="*60)
    
    # 1. æµ‹è¯•æ•°æ®åŠ è½½
    print("1. æµ‹è¯•GQAæ•°æ®åŠ è½½...")
    try:
        gqa_loader = GQALoader(gqa_root="/home/Dataset/Dataset/GQA")
        dataset = gqa_loader.load_dataset(split="train_balanced", num_samples=2)
        print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} ä¸ªæ ·æœ¬")
        
        # å¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        processed_sample = gqa_loader.process_sample(sample)
        print(f"âœ“ æ ·æœ¬å¤„ç†æˆåŠŸï¼Œå›¾åƒå½¢çŠ¶: {processed_sample['image'].size if processed_sample['image'] else 'None'}")
        
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 2. æµ‹è¯•maskç­–ç•¥
    print("\n2. æµ‹è¯•maskç­–ç•¥...")
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿçš„è§†è§‰token
        batch_size, seq_len, hidden_dim = 1, 576, 4096  # LLaVA-1.5çš„å…¸å‹ç»´åº¦
        visual_tokens = torch.randn(batch_size, seq_len, hidden_dim)
        
        # æµ‹è¯•éšæœºmaskç­–ç•¥ - åœ¨num_tokensç»´åº¦ä¸Šæ“ä½œ
        mask_strategy = RandomMaskStrategy(mask_ratio=0.2)
        
        # è·å–maskç´¢å¼• - å¯¹æ¯ä¸ªæ ·æœ¬çš„tokenç»´åº¦è¿›è¡Œmask
        batch_size, num_tokens, hidden_size = visual_tokens.shape
        masked_tokens = visual_tokens.clone()
        
        for i in range(batch_size):
            # å¯¹å•ä¸ªæ ·æœ¬çš„tokenç»´åº¦è¿›è¡Œmaskæ“ä½œ
            single_sample_tokens = visual_tokens[i]  # [num_tokens, hidden_size]
            mask_indices = mask_strategy.get_mask_indices(single_sample_tokens)
            
            # åº”ç”¨mask - å°†é€‰ä¸­çš„æ•´ä¸ªtokenç½®é›¶
            masked_tokens[i][mask_indices] = 0.0
        
        print(f"âœ“ åŸå§‹tokenå½¢çŠ¶: {visual_tokens.shape}")
        print(f"âœ“ Maskåtokenå½¢çŠ¶: {masked_tokens.shape}")
        print(f"âœ“ Maskç´¢å¼•æ•°é‡: {len(mask_indices)}")
        print(f"âœ“ Maskæ¯”ä¾‹: {len(mask_indices) / seq_len:.3f}")
        
        # éªŒè¯maskæ˜¯å¦æ­£ç¡®åº”ç”¨
        original_values = visual_tokens[0, mask_indices, :]
        masked_values = masked_tokens[0, mask_indices, :]
        is_masked = torch.allclose(masked_values, torch.zeros_like(masked_values))
        print(f"âœ“ MaskéªŒè¯: {'é€šè¿‡' if is_masked else 'å¤±è´¥'}")
        
    except Exception as e:
        print(f"âœ— Maskç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–
    print("\n3. æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–...")
    try:
        from llava.utils.attention_visualizer import AttentionVisualizer
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡
        attention_weights = torch.randn(1, 12, 576, 576)  # [batch, heads, seq_len, seq_len]
        
        visualizer = AttentionVisualizer("./test_attention")
        visualizer.visualize_attention_heatmap(
            attention_weights,
            layer_idx=0,
            head_idx=0,
            title="æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–"
        )
        print("âœ“ æ³¨æ„åŠ›å¯è§†åŒ–æµ‹è¯•æˆåŠŸ")
        
    except Exception as e:
        print(f"âœ— æ³¨æ„åŠ›å¯è§†åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\n" + "="*60)
    print("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Pipelineå·¥ä½œæ­£å¸¸")
    print("="*60)
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    success = test_simple_pipeline()
    
    if success:
        print("\nğŸ‰ æ­å–œï¼æ‚¨çš„LLaVA-1.5è§†è§‰token maskæ¶ˆèå®éªŒç¯å¢ƒå·²ç»å‡†å¤‡å°±ç»ªï¼")
        print("\nä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œå®Œæ•´çš„è¯„ä¼°:")
        print("python eval_gqa_masked.py \\")
        print("    --model_path liuhaotian/llava-v1.5-7b \\")
        print("    --gqa_root /home/Dataset/Dataset/GQA \\")
        print("    --gqa_split train_balanced \\")
        print("    --num_samples 10 \\")
        print("    --mask_visual_token \\")
        print("    --mask_ratio 0.2 \\")
        print("    --mask_strategy random")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()
