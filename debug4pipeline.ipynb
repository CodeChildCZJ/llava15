{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255e4866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-22 16:11:00,773] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, torch, random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Âõ∫ÂÆöÈöèÊú∫ÁßçÂ≠ê\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "sys.path.append(\"/home/czj/llava15_test/LLaVA\")\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.data.gqa_loader import GQALoader\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcb6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gqa_eval.config import get_args\n",
    "from gqa_eval.seed_utils import set_seed\n",
    "from gqa_eval.model_loader import load_llava_model\n",
    "from llava.data.gqa_loader import GQALoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1ce4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "def get_fixed_args():\n",
    "    # ÂàõÂª∫‰∏Ä‰∏™ Namespace ÂØπË±°ÔºåÊâãÂä®ËÆæÁΩÆÈªòËÆ§ÂÄºÔºàÊàñ‰Ω†ÊÉ≥Ë¶ÅÁöÑÂÄºÔºâ\n",
    "    args = argparse.Namespace(\n",
    "        # Ê®°ÂûãÂä†ËΩΩ\n",
    "        model_path=\"/home/czj/llava15_test/llava-v1.5-7b\",\n",
    "        device=\"cuda\",\n",
    "        dtype=\"float16\",\n",
    "        load_in_4bit=False,\n",
    "        load_in_8bit=False,\n",
    "\n",
    "        # Ë∂ÖÂèÇÊï∞\n",
    "        batch_size=2,\n",
    "        num_workers=8,\n",
    "        max_new_tokens=64,\n",
    "\n",
    "        # GPUÂπ∂Ë°å\n",
    "        multi_gpu=False,\n",
    "\n",
    "        # maskÈÄâÈ°π\n",
    "        mask_visual_token=False,\n",
    "        mask_ratio=0.1,\n",
    "        mask_strategy=\"random\",\n",
    "        mask_token_value=0.0,\n",
    "\n",
    "        # Êï∞ÊçÆÈõÜ\n",
    "        dataset_path=\"/home/Dataset/Dataset/GQA\",\n",
    "        split=\"testdev\",\n",
    "        max_samples=None,\n",
    "        conv_mode=\"llava_v1\",\n",
    "\n",
    "        # ËæìÂá∫\n",
    "        output_dir=\"./eval_GQA_results/testdev_VQA_100samples\",\n",
    "        save_json=True,\n",
    "        save_pred_gt=True,\n",
    "\n",
    "        seed=42\n",
    "    )\n",
    "    return args\n",
    "args = get_fixed_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e69b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰ΩøÁî®Êú¨Âú∞ GQA Êï∞ÊçÆÈõÜË∑ØÂæÑ: /home/Dataset/Dataset/GQA\n"
     ]
    }
   ],
   "source": [
    "gqa_loader = GQALoader(args.dataset_path, num_workers=args.num_workers, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f831d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Âä†ËΩΩÊ®°Âûã: /home/czj/llava15_test/llava-v1.5-7b\n",
      "ËÆæÂ§á: cuda | dtype=float16 | multi_gpu=False\n",
      "mask_visual_token=False, strategy=random\n",
      "============================================================\n",
      "Ê®°ÂûãÁ±ªÂûã: LLaVA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, image_processor = load_llava_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1885ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from gqa_eval.pred_gt_match import compute_match_batch\n",
    "from gqa_eval.prompt import build_multimodal_batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae28262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split=testdev, batch_size=2, num_workers=8\n",
      "Âä†ËΩΩÊú¨Âú∞ GQA split: testdev\n",
      "cache_path: /home/Dataset/Dataset/GQA/.gqa_cache/testdev_arrow\n",
      "use_cache: True\n",
      "os.path.exists(cache_path): True\n",
      "‰ªé Arrow ÁºìÂ≠òÂä†ËΩΩ: /home/Dataset/Dataset/GQA/.gqa_cache/testdev_arrow\n",
      "Â∑≤Âä†ËΩΩ 100 Êù°Ê†∑Êú¨ÔºàÊù•Ëá™ÁºìÂ≠òÔºâ\n",
      "DataLoader Â∞±Áª™: batch_size=2, num_workers=8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating split={args.split}, batch_size={gqa_loader.batch_size}, num_workers={gqa_loader.num_workers}\")\n",
    "dataloader = gqa_loader.as_dataloader(args.split, args.max_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee158b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = model.module if hasattr(model, \"module\") else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becacd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f28276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model_to_use.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8570fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17aa404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', '</s>', 0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.eos_token, model_to_use.config.pad_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af0bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "tmp_tokenizer_pad_token = deepcopy(tokenizer.pad_token)\n",
    "tmp_tokenizer_eos_token = deepcopy(tokenizer.eos_token)\n",
    "tmp_model_pad_token_id = deepcopy(model_to_use.config.pad_token_id)\n",
    "tmp_tokenizer_pad_token_id = deepcopy(tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5ac39e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', '</s>', 2, 2)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token, tokenizer.eos_token, model_to_use.config.pad_token_id, tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d66938a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tmp_tokenizer_pad_token\n",
    "tokenizer.eos_token = tmp_tokenizer_eos_token\n",
    "model_to_use.config.pad_token_id = tmp_model_pad_token_id\n",
    "tokenizer.pad_token_id = tmp_tokenizer_pad_token_id\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ce3b4c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 'right')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_use.config.pad_token_id, tokenizer.pad_token_id, tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92f8fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  7251,     0],\n",
      "        [    1, 22172,  3186]])\n",
      "tensor([[1, 1, 0],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer([\"hi\", \"hello world\"], padding=True, return_tensors=\"pt\")\n",
    "print(encoded[\"input_ids\"])\n",
    "print(encoded[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e350211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_to_use.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d6fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b47444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "904ada62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 0, 0, 0, 1, 285, 2707, 366], [1, 14051, 29890, 728, 528, 277, 596, 16823]], 'attention_mask': [[0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "p = [\"fuck you\", \"rubbish shit your mom\"]\n",
    "tokenizer(p, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbfb590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52a6f430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GQA:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_batch: tensor([[    0,     0,     1,   319, 13563,  1546,   263, 12758,  5199,   322,\n",
      "           385, 23116, 21082, 20255, 29889,   450, 20255,  4076,  8444, 29892,\n",
      "         13173, 29892,   322,  1248,   568,  6089,   304,   278,  5199, 29915,\n",
      "         29879,  5155, 29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,\n",
      "          3624,   372,   975,  4384, 29973,   319,  1799,  9047, 13566, 29901],\n",
      "        [    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13, 22110,   338,\n",
      "           591,  4362,   278, 10714, 29973,   319,  1799,  9047, 13566, 29901]])\n",
      "\n",
      "üîç Debug: Batch vs Single\n",
      "Question: Is it overcast?\n",
      "GT:       no\n",
      "Batch:    '. no, it is not overcast. the sky is blue, indicating a clear and sunny day.'\n",
      "Single:   'no, it is not overcast. the sky is blue, indicating a clear and sunny day.'\n",
      "Match?    False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GQA:   0%|          | 0/50 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# gqa_eval/prompt.py\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\n",
    "\n",
    "from llava.conversation import conv_templates\n",
    "# from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "def tokenizer_image_token(\n",
    "    prompts,\n",
    "    tokenizer,\n",
    "    image_token_index=IMAGE_TOKEN_INDEX,\n",
    "    return_tensors=None,\n",
    "    padding_side=\"right\",  # \"left\" or \"right\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Â∞ÜÊñáÊú¨‰∏≠ <image> ÊõøÊç¢‰∏∫ image_token_indexÔºåÂπ∂ËøõË°åÂèØÈÄâÁöÑ batch padding„ÄÇ\n",
    "    - ÊîØÊåÅÂçï‰∏™Â≠óÁ¨¶‰∏≤ÊàñÂ≠óÁ¨¶‰∏≤ÂàóË°®„ÄÇ\n",
    "    - ÊîØÊåÅÂ∑¶/Âè≥ padding„ÄÇ\n",
    "    - ‰øùËØÅËøîÂõûÊ†ºÂºèÁªü‰∏Ä„ÄÇ\n",
    "    \"\"\"\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    tokenizer.padding_side = padding_side\n",
    "\n",
    "    all_input_ids = []\n",
    "    for prompt in prompts:\n",
    "        # Êåâ <image> ÊãÜÂàÜ\n",
    "        prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n",
    "        \n",
    "        # ÊèíÂÖ• <image> Âç†‰ΩçÁ¨¶\n",
    "        def insert_separator(X, sep):\n",
    "            return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n",
    "\n",
    "        input_ids = []\n",
    "        offset = 0\n",
    "        if (\n",
    "            len(prompt_chunks) > 0\n",
    "            and len(prompt_chunks[0]) > 0\n",
    "            and prompt_chunks[0][0] == tokenizer.bos_token_id\n",
    "        ):\n",
    "            offset = 1\n",
    "            input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "        for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "            input_ids.extend(x[offset:])\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "    \n",
    "    # ‰∏çÈúÄË¶Å tensorÔºåÁõ¥Êé•ËøîÂõû list[list[int]]\n",
    "    if return_tensors is None:\n",
    "        return all_input_ids\n",
    "\n",
    "    # Âê¶ÂàôËøîÂõû tensor Âπ∂Ëá™Âä® padding\n",
    "    if return_tensors == \"pt\":\n",
    "        padded = tokenizer.pad(\n",
    "            {\"input_ids\": all_input_ids},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return padded\n",
    "\n",
    "    raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(question: str, conv_mode: str = \"llava_v1\") -> str:\n",
    "    \"\"\"ÊûÑÈÄ†ÂçïÊ†∑Êú¨ prompt\"\"\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "\n",
    "def build_multimodal_batch_inputs_new(\n",
    "    batch, tokenizer, image_processor, device, dtype=torch.float16, conv_mode=\"llava_v1\", max_workers=8\n",
    "):\n",
    "    \"\"\"\n",
    "    ÊâπÈáèÊûÑÂª∫Â§öÊ®°ÊÄÅËæìÂÖ• (ÊñáÊú¨ + ÂõæÂÉè)\n",
    "    - ÊñáÊú¨ÈÉ®ÂàÜÊâπÈáè tokenizer padding\n",
    "    - ÂõæÂÉèÈÉ®ÂàÜÂ§öÁ∫øÁ®ã image_processor È¢ÑÂ§ÑÁêÜ\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 ÊñáÊú¨ÊâπÈáèÁîüÊàê prompt\n",
    "    prompts = []\n",
    "    for s in batch:\n",
    "        q = s[\"question\"]\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + q)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompts.append(conv.get_prompt())\n",
    "\n",
    "    # 2 Tokenizer ÊâπÈáèÁºñÁ†ÅÔºàËá™Âä® paddingÔºâ\n",
    "\n",
    "\n",
    "    tok = tokenizer_image_token(\n",
    "        prompts,\n",
    "        tokenizer,\n",
    "        IMAGE_TOKEN_INDEX,\n",
    "        return_tensors=\"pt\",\n",
    "        padding_side=tokenizer.padding_side # Êàñ \"left\"\n",
    "    )\n",
    "    # print(tok[\"input_ids\"])\n",
    "    input_ids = tok[\"input_ids\"]\n",
    "    attention_mask = tok[\"attention_mask\"]\n",
    "\n",
    "    # image = batch[0][\"image\"]\n",
    "    # image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device, dtype)\n",
    "    # print(image_tensor)\n",
    "\n",
    "    # 3 ÂõæÂÉèÂπ∂Ë°åÈ¢ÑÂ§ÑÁêÜ\n",
    "    def _process_img(s):\n",
    "        image = s.get(\"image\", None)\n",
    "        if image is None:\n",
    "            return None\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        try:\n",
    "            return image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device, dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] ÂõæÂÉèÂ§ÑÁêÜÂ§±Ë¥•: {e}\")\n",
    "            return None\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=min(max_workers, len(batch))) as ex:\n",
    "        image_tensors = list(ex.map(_process_img, batch))   \n",
    "\n",
    "    image_sizes = [s[\"image\"].size for s in batch]\n",
    "    # print(input_ids, image_tensors)\n",
    "    return input_ids, attention_mask, torch.cat(image_tensors, dim=0), image_sizes\n",
    "\n",
    "\n",
    "\n",
    "counters = {k: Counter() for k in [\"llava_match\", \"lmms_match\", \"loose_match\"]}\n",
    "correct_loose, total = 0, 0\n",
    "results = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Evaluating GQA\"):\n",
    "    # ============================================================\n",
    "    # ÊûÑÂª∫ÊâπÈáèËæìÂÖ•ÔºàÊñáÊú¨+ÂõæÂÉèÔºâ\n",
    "    # ============================================================\n",
    "\n",
    "\n",
    "    input_ids_batch, attention_mask_batch, image_tensors, image_sizes = build_multimodal_batch_inputs_new(\n",
    "        batch, tokenizer, image_processor, args.device, torch.float16, args.conv_mode\n",
    "    )\n",
    "\n",
    "    print(f\"input_ids_batch: {input_ids_batch}\")\n",
    "    # print(f\"attention_mask_batch: {attention_mask_batch}\")\n",
    "\n",
    "    # Ground truth ‰∏éÈóÆÈ¢ò\n",
    "    gts = [s[\"answer\"].strip().lower() for s in batch if s.get(\"answer\")]\n",
    "    questions = [s[\"question\"] for s in batch]\n",
    "\n",
    "    # ============================================================\n",
    "    # Ê®°ÂûãÁîüÊàê\n",
    "    # ============================================================\n",
    "\n",
    "    outputs = model_to_use.generate(\n",
    "        inputs=input_ids_batch.to(args.device),\n",
    "        images=image_tensors.to(args.device),\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        attention_mask=attention_mask_batch.to(args.device),\n",
    "        image_sizes=image_sizes,\n",
    "        temperature=0,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    preds = [p.strip().lower() for p in preds]\n",
    "\n",
    "    # ============================================================\n",
    "    # ÊâπÈáèÂåπÈÖç + ÂêëÈáèÂåñÁªüËÆ°\n",
    "    # ============================================================\n",
    "    batch_matches = compute_match_batch(preds, gts)\n",
    "\n",
    "    llava_match = np.fromiter((m[\"llava_match\"] for m in batch_matches), dtype=bool)\n",
    "    lmms_match = np.fromiter((m[\"lmms_match\"] for m in batch_matches), dtype=bool)\n",
    "    loose_match = np.fromiter((m[\"loose_match\"] for m in batch_matches), dtype=bool)\n",
    "\n",
    "    is_correct = lmms_match | loose_match\n",
    "\n",
    "    # ÂêëÈáèÂåñÁªüËÆ°Êõ¥Êñ∞\n",
    "    counters[\"llava_match\"].update({True: llava_match.sum(), False: len(llava_match) - llava_match.sum()})\n",
    "    counters[\"lmms_match\"].update({True: lmms_match.sum(), False: len(lmms_match) - lmms_match.sum()})\n",
    "    counters[\"loose_match\"].update({True: loose_match.sum(), False: len(loose_match) - loose_match.sum()})\n",
    "\n",
    "    correct_loose += int(is_correct.sum())\n",
    "    total += len(is_correct)\n",
    "    # print(questions, gts)\n",
    "\n",
    "    def single_inference(image, idx):\n",
    "\n",
    "        single_prompt = build_prompt(questions[idx], args.conv_mode)\n",
    "        tok = tokenizer_image_token([single_prompt], tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        single_input_ids = tok[\"input_ids\"].to(args.device)          # shape (1, L)\n",
    "        single_attention_mask = tok[\"attention_mask\"].to(args.device)\n",
    "\n",
    "        batch_pred = preds[idx]\n",
    "        single_output = model_to_use.generate(\n",
    "            inputs=single_input_ids,\n",
    "            images=image.unsqueeze(0),\n",
    "            image_sizes=[image.size],\n",
    "            attention_mask=single_attention_mask,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            temperature=0,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # print(f\"single_prompt: {single_prompt}\")\n",
    "        # # print(f\"batch_prompt: {tokenizer.decode(input_ids_batch[idx], skip_special_tokens=True)}\")\n",
    "\n",
    "        # print(f\"batch_input_ids: {input_ids_batch[idx]}\")\n",
    "        # print(f\"single_input_ids: {single_input_ids}\")\n",
    "\n",
    "        # print(f\"batch_output: {outputs[idx]}\")\n",
    "        # print(f\"single_output: {single_output}\")\n",
    "\n",
    "        # print(f\"batch_decode: {tokenizer.batch_decode(outputs, skip_special_tokens=True)[idx]}\")\n",
    "        # print(f\"single_decode: {tokenizer.decode(single_output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "        single_pred = tokenizer.decode(single_output[0], skip_special_tokens=True).strip().lower()\n",
    "        if batch_pred != single_pred:\n",
    "\n",
    "            print(\"\\nüîç Debug: Batch vs Single\")\n",
    "            print(f\"Question: {questions[idx]}\")\n",
    "            print(f\"GT:       {gts[idx]}\")\n",
    "            print(f\"Batch:    '{batch_pred}'\")\n",
    "            print(f\"Single:   '{single_pred}'\")\n",
    "            print(f\"Match?    {batch_pred == single_pred}\")\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        single_inference(image_tensors[i], i)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2efdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutputWithPast"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3181a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "match1_testdev_GQA_100samples_bs2_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs2/pred_gt.json\"\n",
    "match1_testdev_GQA_100samples_bs2_data = read_json(match1_testdev_GQA_100samples_bs2_path)\n",
    "\n",
    "testdev_GQA_100samples_bs2_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs2/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs2_data = read_json(testdev_GQA_100samples_bs2_path)\n",
    "\n",
    "testdev_GQA_100samples_bs2_list_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs2_list/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs2_list_data = read_json(testdev_GQA_100samples_bs2_list_path)\n",
    "\n",
    "def get_data(data):\n",
    "    result = [  (d[\"question\"], d[\"pred\"], d[\"gt\"]) for d in data]\n",
    "    return result\n",
    "\n",
    "match1, b2, b2l = get_data(match1_testdev_GQA_100samples_bs2_data), get_data(testdev_GQA_100samples_bs2_data), get_data(testdev_GQA_100samples_bs2_list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0ab555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is it overcast?', '', 'no', False),\n",
       " ('Who is wearing the dress?',\n",
       "  'the image shows a woman wearing a dress.',\n",
       "  'women',\n",
       "  False),\n",
       " ('Does the utensil on top of the table look clean and black?',\n",
       "  'yes, the utensil on top of the table looks clean and black.',\n",
       "  'no',\n",
       "  False)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match1_bs2_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs2/pred_gt.json\"\n",
    "match1_bs8_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs8/pred_gt.json\"\n",
    "\n",
    "def get_data_with_correct(data):\n",
    "    result = [  (d[\"question\"], d[\"pred\"], d[\"gt\"], d[\"is_correct\"]) for d in data]\n",
    "    return result\n",
    "\n",
    "mbs2, mbs8 = get_data_with_correct(read_json(match1_bs2_path)), get_data_with_correct(read_json(match1_bs8_path))\n",
    "mbs2[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c758585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question: Is the ground blue or brown?, gt: brown, match1: ('the ground in the image is brown.',), match2: \n",
      " question: Is the jacket long sleeved and black?, gt: yes, match1: ('yes, the jacket is long sleeved and black.',), match2: \n",
      " question: Are there beds next to the small outlet?, gt: yes, match1: ('yes, there are beds next to the small outlet.',), match2: \n",
      " question: Are the plates on top of an ottoman?, gt: no, match1: ('no, the plates are not on top of an ottoman. they are on top of a table.',), match2: \n",
      " question: Which side are the white houses on?, gt: left, match1: ('left',), match2: \n",
      " question: Are there either any small refrigerators or microwaves in the picture?, gt: no, match1: ('no, there are no small refrigerators or microwaves in the picture. the image shows a large refrigerator and a microwave.',), match2: yes, there is a small refrigerator and a small microwave in the picture.\n",
      " question: How does that car look like, orange or maybe white?, gt: white, match1: ('the car in the image is white.',), match2: \n",
      " question: What are the drapes around of?, gt: window, match1: ('the drapes are around a window.',), match2: \n",
      " question: On which side is the picture?, gt: left, match1: ('the picture is on the left side of the image.',), match2: \n",
      " question: Is this helicopter on or off?, gt: off, match1: ('the helicopter in the image is off.',), match2: ,\n",
      " question: Do you see any cats?, gt: yes, match1: ('yes, there is a cat in the image.',), match2: \n",
      " question: Is that shoe behind a dog?, gt: no, match1: ('',), match2: no\n",
      " question: Is the field soft and snowy?, gt: no, match1: ('yes, the field is soft and snowy.',), match2: \n",
      " question: Which color is the shirt?, gt: white, match1: ('the shirt is white.',), match2: \n",
      " question: Is there a pear beneath the appliance that looks silver and black?, gt: no, match1: ('no, there is no pear beneath the silver and black appliance. the image shows a silver and black appliance sitting on top of a table, but there is no pear or any other object beneath it.',), match2: \n",
      " question: On which side is the router?, gt: left, match1: ('the router is on the left side of the image.',), match2: router\n",
      " question: What's the man doing?, gt: standing, match1: ('the man in the image is standing in front of a building, possibly a church, and appears to be looking up at the sky. it is not possible to determine his exact actions or intentions from the image alone.',), match2: \n",
      " question: Is the van in front of a balloon?, gt: no, match1: ('no, the van is not in front of a balloon. the image shows a van driving down a street, but there is no indication of a balloon being present.',), match2: yes, the van is in front of a balloon.\n",
      " question: Are the boxes to the right of the man full and square?, gt: yes, match1: ('yes, the boxes to the right of the man are full and square.',), match2: (1) no, the boxes to the right of the man are empty and square.\n",
      " question: Does the blanket look soft and white?, gt: yes, match1: ('yes, the blanket looks soft and white.',), match2: \n",
      " question: How wide is the parking lot made of cement?, gt: wide, match1: ('the parking lot made of cement is quite wide, as it can accommodate multiple vehicles parked side by side.',), match2: \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mbs2)):\n",
    "    a, b = mbs2[i], mbs8[i]\n",
    "    assert(a[0] == b[0] and a[2] == b[2])\n",
    "    if a[3] != b[3]:\n",
    "        print(f' question: {a[0]}, gt: {a[2]}, match1: {a[1], }, match2: {b[1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18c02d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is it overcast?', '', 'no'),\n",
       " ('Who is wearing the dress?',\n",
       "  'the image shows a woman wearing a dress.',\n",
       "  'women'),\n",
       " ('Does the utensil on top of the table look clean and black?',\n",
       "  'yes, the utensil on top of the table looks clean and black.',\n",
       "  'no'),\n",
       " ('Is the surfer that looks wet wearing a wetsuit?', '(no)', 'yes'),\n",
       " ('How tall is the chair in the bottom of the photo?',\n",
       "  'the chair in the bottom of the photo is very tall.',\n",
       "  'short')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ba5559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match1_testdev_GQA_100samples_bs8_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs8/pred_gt.json\"\n",
    "match1_testdev_GQA_100samples_bs8_data = read_json(match1_testdev_GQA_100samples_bs8_path)\n",
    "\n",
    "testdev_GQA_100samples_bs8_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs8/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs8_data = read_json(testdev_GQA_100samples_bs8_path)\n",
    "\n",
    "testdev_GQA_100samples_bs8_list_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs8_list/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs8_list_data = read_json(testdev_GQA_100samples_bs8_list_path)\n",
    "\n",
    "match2, b8, b8l = get_data(match1_testdev_GQA_100samples_bs8_data), get_data(testdev_GQA_100samples_bs8_data), get_data(testdev_GQA_100samples_bs8_list_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f68a73d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match2 == b8, match2 == b8l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a93ef704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is it overcast?', '', 'no'),\n",
       " ('Who is wearing the dress?', '', 'women'),\n",
       " ('Does the utensil on top of the table look clean and black?',\n",
       "  'yes, the utensil on top of the table looks clean and black.',\n",
       "  'no'),\n",
       " ('Is the surfer that looks wet wearing a wetsuit?', '(no)', 'yes'),\n",
       " ('How tall is the chair in the bottom of the photo?', '', 'short')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70050a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question: Who is wearing the dress?, gt: women, match1: ('the image shows a woman wearing a dress.',), match2: \n",
      " question: How tall is the chair in the bottom of the photo?, gt: short, match1: ('the chair in the bottom of the photo is very tall.',), match2: \n",
      " question: What kind of device is on top of the desk?, gt: keyboard, match1: ('there is a laptop computer on top of the desk.',), match2: \n",
      " question: What is the airplane flying above?, gt: ocean, match1: ('the airplane is flying above a city.',), match2: \n",
      " question: What color are the pants?, gt: red, match1: ('<image>',), match2: \n",
      " question: Is the ground blue or brown?, gt: brown, match1: ('the ground in the image is brown.',), match2: \n",
      " question: What is around the open window?, gt: drapes, match1: ('there is a chair placed next to the open window.',), match2: \n",
      " question: What's around the window?, gt: drapes, match1: ('there is a white frame around the window.',), match2: \n",
      " question: Who is standing at the table?, gt: woman, match1: ('a man is standing at the table.',), match2: \n",
      " question: Are there drapes to the right of the bed?, gt: yes, match1: ('no, there are no drapes to the right of the bed.',), match2: \n",
      " question: What is hanging above the chalkboard?, gt: picture, match1: ('is a chalkboard.',), match2: a chalkboard\n",
      " question: What device is sitting next to the mouse pad?, gt: keyboard, match1: ('a computer mouse is sitting next to the mouse pad.',), match2: \n",
      " question: Does the sweater look open and blue?, gt: yes, match1: (\"'\",), match2: \n",
      " question: Is the jacket long sleeved and black?, gt: yes, match1: ('yes, the jacket is long sleeved and black.',), match2: \n",
      " question: Are there beds next to the small outlet?, gt: yes, match1: ('yes, there are beds next to the small outlet.',), match2: \n",
      " question: On which side of the picture is the leather bag?, gt: right, match1: ('the leather bag is on the left side of the picture.',), match2: \n",
      " question: Is the blue pillow square and large?, gt: no, match1: ('ure, the blue pillow is square and large.',), match2: \n",
      " question: Are the plates on top of an ottoman?, gt: no, match1: ('no, the plates are not on top of an ottoman. they are on top of a table.',), match2: \n",
      " question: Is the fence made of cement or aluminum?, gt: aluminum, match1: ('the fence in the image is made of cement.',), match2: \n",
      " question: Which side are the white houses on?, gt: left, match1: ('left',), match2: \n",
      " question: Are there either any small refrigerators or microwaves in the picture?, gt: no, match1: ('no, there are no small refrigerators or microwaves in the picture. the image shows a large refrigerator and a microwave.',), match2: yes, there is a small refrigerator and a small microwave in the picture.\n",
      " question: How does that car look like, orange or maybe white?, gt: white, match1: ('the car in the image is white.',), match2: \n",
      " question: Is the bag made of leather lying on top of a sofa?, gt: no, match1: ('no, the bag made of leather is not lying on top of a sofa. it is placed on the ground.',), match2: no\n",
      " question: What are the drapes around of?, gt: window, match1: ('the drapes are around a window.',), match2: \n",
      " question: On which side is the picture?, gt: left, match1: ('the picture is on the left side of the image.',), match2: \n",
      " question: How is the clothing item that is pink called?, gt: tank top, match1: ('the clothing item that is pink is a dress.',), match2: \n",
      " question: Which kind of clothing is not pink?, gt: hat, match1: ('is not pink',), match2: shirt\n",
      " question: Is this helicopter on or off?, gt: off, match1: ('the helicopter in the image is off.',), match2: ,\n",
      " question: Do you see any cats?, gt: yes, match1: ('yes, there is a cat in the image.',), match2: \n",
      " question: Is that shoe behind a dog?, gt: no, match1: ('',), match2: no\n",
      " question: What kind of clothing is sleeveless?, gt: tank top, match1: ('the image shows a person wearing a sleeveless shirt.',), match2: leeveless shirt\n",
      " question: Is the field soft and snowy?, gt: no, match1: ('yes, the field is soft and snowy.',), match2: \n",
      " question: What fruits are beneath the microwave?, gt: bananas, match1: ('there are no fruits beneath the microwave in the image. the image shows a microwave sitting on top of a counter or table, with nothing else beneath it.',), match2: \n",
      " question: Which color is the shirt?, gt: white, match1: ('the shirt is white.',), match2: \n",
      " question: What is beneath the microwave?, gt: bananas, match1: ('beneath the microwave, there is a countertop.',), match2: \n",
      " question: Is there a pear beneath the appliance that looks silver and black?, gt: no, match1: ('no, there is no pear beneath the silver and black appliance. the image shows a silver and black appliance sitting on top of a table, but there is no pear or any other object beneath it.',), match2: \n",
      " question: Is this a bed or a cabinet?, gt: cabinet, match1: ('this is a bed.',), match2: orry, i'm not sure what you're asking. could you please provide more context or clarify your question?\n",
      " question: On which side is the router?, gt: left, match1: ('the router is on the left side of the image.',), match2: router\n",
      " question: What is the color of the pants?, gt: gray, match1: ('the color of the pants is black.',), match2: \n",
      " question: Who is wearing a shirt?, gt: girl, match1: ('a person is wearing a shirt.',), match2: \n",
      " question: What's the man doing?, gt: standing, match1: ('the man in the image is standing in front of a building, possibly a church, and appears to be looking up at the sky. it is not possible to determine his exact actions or intentions from the image alone.',), match2: \n",
      " question: Which kind of vehicle is in front of the flag?, gt: van, match1: ('a car is in front of the flag.',), match2: <image>\n",
      " question: What is the vehicle that is in front of the flag?, gt: van, match1: ('the vehicle in front of the flag is a truck.',), match2: </image>\n",
      " question: Is the van in front of a balloon?, gt: no, match1: ('no, the van is not in front of a balloon. the image shows a van driving down a street, but there is no indication of a balloon being present.',), match2: yes, the van is in front of a balloon.\n",
      " question: How is the item of furniture that is plaid called?, gt: bed, match1: ('the item of furniture that is plaid is called a sofa or a couch. both terms are commonly used to describe a long, narrow seat with a back and armrests, typically used for seating multiple people. the pattern of the fabric on the sofa or couch is called plaid,',), match2: (1) the item of furniture that is plaid is called a couch.\n",
      "(2) the furniture that is plaid is a couch.\n",
      "(3) the couch is plaid in color.\n",
      "(4) the couch is a piece of furniture.\n",
      "(5\n",
      " question: Are the boxes to the right of the man full and square?, gt: yes, match1: ('yes, the boxes to the right of the man are full and square.',), match2: (1) no, the boxes to the right of the man are empty and square.\n",
      " question: Is the horse next to the other horse both baby and brown?, gt: no, match1: ('yes, the horse next to the other horse is both a baby and brown.',), match2: (1) the horse next to the other horse is both baby and brown.\n",
      "\n",
      "(2) the horse next to the other horse is a baby brown horse.\n",
      "\n",
      "(3) the horse next to the other horse is a brown baby horse.\n",
      "\n",
      "(4) the horse next to the other horse\n",
      " question: What appliance is the refrigerator larger than?, gt: stove, match1: ('the refrigerator is larger than the oven.',), match2: <image>\n",
      " question: What is the color of the glove?, gt: black, match1: ('the color of the glove is white.',), match2: \n",
      " question: Does the blanket look soft and white?, gt: yes, match1: ('yes, the blanket looks soft and white.',), match2: \n",
      " question: Are there refrigerators to the left of the stove?, gt: yes, match1: ('no, there are no refrigerators to the left of the stove.',), match2: <image>\n",
      " question: Which type of clothing is pink?, gt: gown, match1: ('is wearing a pink shirt.',), match2: wear\n",
      " question: What is the person that is sitting down sitting atop?, gt: stairs, match1: ('the person sitting down is sitting atop a pile of books.',), match2: \n",
      " question: What items of furniture are to the left of the boy?, gt: tables, match1: ('there are two chairs to the left of the boy.',), match2: (1) a bookshelf and (2) a desk are to the left of the boy.\n",
      " question: What is in front of the wall that is not short?, gt: shelf, match1: ('there is a chair in front of the wall that is not short.',), match2: \n",
      " question: How wide is the parking lot made of cement?, gt: wide, match1: ('the parking lot made of cement is quite wide, as it can accommodate multiple vehicles parked side by side.',), match2: \n",
      " question: What is the device in front of the flat computer?, gt: phone, match1: ('there is a flat computer monitor in front of the device.',), match2: the device in front of the flat computer is a tablet.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(match2)):\n",
    "    a, b = match1[i], match2[i]\n",
    "    assert(a[0] == b[0] and a[2] == b[2])\n",
    "    if a != b:\n",
    "        print(f' question: {a[0]}, gt: {a[2]}, match1: {a[1], }, match2: {b[1]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73264c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292dcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‰ªéÁºìÂ≠òÂä†ËΩΩÊï∞ÊçÆÊ†∑Êú¨...\n",
      "ÁºìÂ≠òÊ†∑Êú¨Êï∞: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'imageId', 'question', 'answer', 'fullAnswer', 'types', 'groups', 'semantic', 'structural', 'image_data', 'image'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "CACHE_FILE = \"gqa_cache.pkl\"\n",
    "if Path(CACHE_FILE).exists():\n",
    "    print(\"‚úÖ ‰ªéÁºìÂ≠òÂä†ËΩΩÊï∞ÊçÆÊ†∑Êú¨...\")\n",
    "    with open(CACHE_FILE, \"rb\") as f:\n",
    "        dataset_cache = pickle.load(f)\n",
    "else:\n",
    "    print(\"üöÄ Á¨¨‰∏ÄÊ¨°Âä†ËΩΩÊï∞ÊçÆÈõÜ...\")\n",
    "    gqa_loader = GQALoader(gqa_root=\"/home/Dataset/Dataset/GQA\")\n",
    "    dataset = gqa_loader.load_dataset(split=\"train_balanced\", num_samples=10)\n",
    "    dataset_cache = [gqa_loader.process_sample(dataset[i]) for i in range(min(10, len(dataset)))]\n",
    "\n",
    "    \n",
    "    # Â≠òÁºìÂ≠ò\n",
    "    with open(CACHE_FILE, \"wb\") as f:\n",
    "        pickle.dump(dataset_cache, f)\n",
    "    print(\"‚úÖ Â∑≤ÁºìÂ≠òÂâç10‰∏™Ê†∑Êú¨Âà∞ gqa_cache.pkl\")\n",
    "\n",
    "print(f\"ÁºìÂ≠òÊ†∑Êú¨Êï∞: {len(dataset_cache)}\")\n",
    "sample = dataset_cache[0]\n",
    "sample.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8218a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama_masked. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ê®°ÂûãÂä†ËΩΩÂÆåÊàê\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/czj/llava15_test/llava-v1.5-7b\"\n",
    "clip_path = \"/home/czj/llava15_test/clip-vit-large-patch14-336\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    device_map=\"cuda:1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_masked_model=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Ê®°ÂûãÂä†ËΩΩÂÆåÊàê\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "858aacc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama_masked.LlavaLlamaForCausalLMMasked"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db2fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ÂéüÂõæÂ∞∫ÂØ∏: (500, 270), tensorÂΩ¢Áä∂: torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset_cache[0]\n",
    "image = sample[\"image\"].convert(\"RGB\")\n",
    "\n",
    "image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(model.device, dtype=torch.float16)\n",
    "\n",
    "print(f\"[DEBUG] ÂéüÂõæÂ∞∫ÂØ∏: {image.size}, tensorÂΩ¢Áä∂: {image_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4357ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÈóÆÈ¢òÔºö Is the sky dark?  Ê®°ÂûãËæìÂá∫: Yes, the sky is dark in the image.\n",
      "ÈóÆÈ¢òÔºö What is on the white wall?  Ê®°ÂûãËæìÂá∫: There is a painting on the white wall.\n",
      "ÈóÆÈ¢òÔºö Is that pipe red?  Ê®°ÂûãËæìÂá∫: No, the pipe is not red. It is a black pipe.\n",
      "ÈóÆÈ¢òÔºö Is the tall clock small or large?  Ê®°ÂûãËæìÂá∫: The tall clock is large, as it is described as a large clock tower.\n",
      "ÈóÆÈ¢òÔºö Who is wearing a shirt?  Ê®°ÂûãËæìÂá∫: A boy is wearing a shirt.\n",
      "ÈóÆÈ¢òÔºö What do you think is he sleeping in?  Ê®°ÂûãËæìÂá∫: The man is sleeping in a bed, which is covered with a white comforter.\n",
      "ÈóÆÈ¢òÔºö Is the cheese to the left of the food on the plate?  Ê®°ÂûãËæìÂá∫: Yes, the cheese is to the left of the food on the plate.\n",
      "ÈóÆÈ¢òÔºö What is the piece of furniture that he is sleeping in?  Ê®°ÂûãËæìÂá∫: The man is sleeping in a bed.\n",
      "ÈóÆÈ¢òÔºö What kind of furniture is to the right of the chair?  Ê®°ÂûãËæìÂá∫: There is a couch to the right of the chair.\n",
      "ÈóÆÈ¢òÔºö Is the steel spatula in the top of the image?  Ê®°ÂûãËæìÂá∫: Yes, the steel spatula is in the top of the image.\n"
     ]
    }
   ],
   "source": [
    "from llava.conversation import conv_templates\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "for sample in dataset_cache:\n",
    "    image = sample[\"image\"].convert(\"RGB\")\n",
    "\n",
    "    image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(model.device, dtype=torch.float16)\n",
    "    question = sample[\"question\"]\n",
    "\n",
    "    conv = conv_templates[\"llava_v1\"].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            inputs=input_ids,\n",
    "            images=[image_tensor],\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            top_p=None,\n",
    "            num_beams=1\n",
    "        )\n",
    "\n",
    "\n",
    "    # answer = tokenizer.batch_decode(output_tokens[0], skip_special_tokens=True)\n",
    "    answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(\"ÈóÆÈ¢òÔºö\", question, \" Ê®°ÂûãËæìÂá∫:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90f37212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mask_visual_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41f6ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object in the image is red.\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# # ËΩΩÂÖ•Ê®°Âûã\n",
    "# tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "#     model_path=model_path,\n",
    "#     model_base=None,\n",
    "#     model_name=get_model_name_from_path(model_path),\n",
    "#     device_map=\"cuda:1\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     # use_masked_model=True\n",
    "# )\n",
    "# ÂõæÂÉè\n",
    "image = Image.open(\"/home/czj/llava15_test/LLaVA/images/llava_v1_5_radar.jpg\").convert(\"RGB\")\n",
    "image_tensor = process_images([image], image_processor, model.config).to(model.device, dtype=torch.float16)\n",
    "\n",
    "# prompt\n",
    "question = \"What is the color of the object in the image?\"\n",
    "conv = conv_templates[\"llava_v1\"].copy()\n",
    "conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "\n",
    "# ÁîüÊàê\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensor,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5e697a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   450,  1203,   297,   278,  1967,   338,  2654, 29889,     2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2faf76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Last hidden shape: torch.Size([1, 623, 4096])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        images=image_tensor,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "print(f\"[DEBUG] Last hidden shape: {outputs.hidden_states[-1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "291233f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Ê®°ÂûãËæìÂá∫: <s>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        inputs=input_ids,\n",
    "        images=image_tensor,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        top_p=None,\n",
    "        num_beams=1\n",
    "    )\n",
    "\n",
    "answer = tokenizer.batch_decode(output_tokens[0], skip_special_tokens=True)[0]\n",
    "print(\"üß© Ê®°ÂûãËæìÂá∫:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
