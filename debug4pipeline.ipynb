{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255e4866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-22 16:11:00,773] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, torch, random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# 固定随机种子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "sys.path.append(\"/home/czj/llava15_test/LLaVA\")\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.data.gqa_loader import GQALoader\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcb6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gqa_eval.config import get_args\n",
    "from gqa_eval.seed_utils import set_seed\n",
    "from gqa_eval.model_loader import load_llava_model\n",
    "from llava.data.gqa_loader import GQALoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1ce4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "def get_fixed_args():\n",
    "    # 创建一个 Namespace 对象，手动设置默认值（或你想要的值）\n",
    "    args = argparse.Namespace(\n",
    "        # 模型加载\n",
    "        model_path=\"/home/czj/llava15_test/llava-v1.5-7b\",\n",
    "        device=\"cuda\",\n",
    "        dtype=\"float16\",\n",
    "        load_in_4bit=False,\n",
    "        load_in_8bit=False,\n",
    "\n",
    "        # 超参数\n",
    "        batch_size=2,\n",
    "        num_workers=8,\n",
    "        max_new_tokens=64,\n",
    "\n",
    "        # GPU并行\n",
    "        multi_gpu=False,\n",
    "\n",
    "        # mask选项\n",
    "        mask_visual_token=False,\n",
    "        mask_ratio=0.1,\n",
    "        mask_strategy=\"random\",\n",
    "        mask_token_value=0.0,\n",
    "\n",
    "        # 数据集\n",
    "        dataset_path=\"/home/Dataset/Dataset/GQA\",\n",
    "        split=\"testdev\",\n",
    "        max_samples=None,\n",
    "        conv_mode=\"llava_v1\",\n",
    "\n",
    "        # 输出\n",
    "        output_dir=\"./eval_GQA_results/testdev_VQA_100samples\",\n",
    "        save_json=True,\n",
    "        save_pred_gt=True,\n",
    "\n",
    "        seed=42\n",
    "    )\n",
    "    return args\n",
    "args = get_fixed_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e69b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用本地 GQA 数据集路径: /home/Dataset/Dataset/GQA\n"
     ]
    }
   ],
   "source": [
    "gqa_loader = GQALoader(args.dataset_path, num_workers=args.num_workers, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f831d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "加载模型: /home/czj/llava15_test/llava-v1.5-7b\n",
      "设备: cuda | dtype=float16 | multi_gpu=False\n",
      "mask_visual_token=False, strategy=random\n",
      "============================================================\n",
      "模型类型: LLaVA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, image_processor = load_llava_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1885ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from gqa_eval.pred_gt_match import compute_match_batch\n",
    "from gqa_eval.prompt import build_multimodal_batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae28262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split=testdev, batch_size=2, num_workers=8\n",
      "加载本地 GQA split: testdev\n",
      "cache_path: /home/Dataset/Dataset/GQA/.gqa_cache/testdev_arrow\n",
      "use_cache: True\n",
      "os.path.exists(cache_path): True\n",
      "从 Arrow 缓存加载: /home/Dataset/Dataset/GQA/.gqa_cache/testdev_arrow\n",
      "已加载 100 条样本（来自缓存）\n",
      "DataLoader 就绪: batch_size=2, num_workers=8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating split={args.split}, batch_size={gqa_loader.batch_size}, num_workers={gqa_loader.num_workers}\")\n",
    "dataloader = gqa_loader.as_dataloader(args.split, args.max_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee158b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = model.module if hasattr(model, \"module\") else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becacd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f28276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model_to_use.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8570fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17aa404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', '</s>', 0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.eos_token, model_to_use.config.pad_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af0bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "tmp_tokenizer_pad_token = deepcopy(tokenizer.pad_token)\n",
    "tmp_tokenizer_eos_token = deepcopy(tokenizer.eos_token)\n",
    "tmp_model_pad_token_id = deepcopy(model_to_use.config.pad_token_id)\n",
    "tmp_tokenizer_pad_token_id = deepcopy(tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5ac39e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', '</s>', 2, 2)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token, tokenizer.eos_token, model_to_use.config.pad_token_id, tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d66938a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tmp_tokenizer_pad_token\n",
    "tokenizer.eos_token = tmp_tokenizer_eos_token\n",
    "model_to_use.config.pad_token_id = tmp_model_pad_token_id\n",
    "tokenizer.pad_token_id = tmp_tokenizer_pad_token_id\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ce3b4c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 'right')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_use.config.pad_token_id, tokenizer.pad_token_id, tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92f8fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  7251,     0],\n",
      "        [    1, 22172,  3186]])\n",
      "tensor([[1, 1, 0],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer([\"hi\", \"hello world\"], padding=True, return_tensors=\"pt\")\n",
    "print(encoded[\"input_ids\"])\n",
    "print(encoded[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e350211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_to_use.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d6fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b47444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "904ada62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 0, 0, 0, 1, 285, 2707, 366], [1, 14051, 29890, 728, 528, 277, 596, 16823]], 'attention_mask': [[0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "p = [\"fuck you\", \"rubbish shit your mom\"]\n",
    "tokenizer(p, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbfb590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52a6f430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GQA:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_batch: tensor([[    0,     0,     1,   319, 13563,  1546,   263, 12758,  5199,   322,\n",
      "           385, 23116, 21082, 20255, 29889,   450, 20255,  4076,  8444, 29892,\n",
      "         13173, 29892,   322,  1248,   568,  6089,   304,   278,  5199, 29915,\n",
      "         29879,  5155, 29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,\n",
      "          3624,   372,   975,  4384, 29973,   319,  1799,  9047, 13566, 29901],\n",
      "        [    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13, 22110,   338,\n",
      "           591,  4362,   278, 10714, 29973,   319,  1799,  9047, 13566, 29901]])\n",
      "\n",
      "🔍 Debug: Batch vs Single\n",
      "Question: Is it overcast?\n",
      "GT:       no\n",
      "Batch:    '. no, it is not overcast. the sky is blue, indicating a clear and sunny day.'\n",
      "Single:   'no, it is not overcast. the sky is blue, indicating a clear and sunny day.'\n",
      "Match?    False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GQA:   0%|          | 0/50 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# gqa_eval/prompt.py\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\n",
    "\n",
    "from llava.conversation import conv_templates\n",
    "# from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "def tokenizer_image_token(\n",
    "    prompts,\n",
    "    tokenizer,\n",
    "    image_token_index=IMAGE_TOKEN_INDEX,\n",
    "    return_tensors=None,\n",
    "    padding_side=\"right\",  # \"left\" or \"right\"\n",
    "):\n",
    "    \"\"\"\n",
    "    将文本中 <image> 替换为 image_token_index，并进行可选的 batch padding。\n",
    "    - 支持单个字符串或字符串列表。\n",
    "    - 支持左/右 padding。\n",
    "    - 保证返回格式统一。\n",
    "    \"\"\"\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    tokenizer.padding_side = padding_side\n",
    "\n",
    "    all_input_ids = []\n",
    "    for prompt in prompts:\n",
    "        # 按 <image> 拆分\n",
    "        prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n",
    "        \n",
    "        # 插入 <image> 占位符\n",
    "        def insert_separator(X, sep):\n",
    "            return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n",
    "\n",
    "        input_ids = []\n",
    "        offset = 0\n",
    "        if (\n",
    "            len(prompt_chunks) > 0\n",
    "            and len(prompt_chunks[0]) > 0\n",
    "            and prompt_chunks[0][0] == tokenizer.bos_token_id\n",
    "        ):\n",
    "            offset = 1\n",
    "            input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "        for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "            input_ids.extend(x[offset:])\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "    \n",
    "    # 不需要 tensor，直接返回 list[list[int]]\n",
    "    if return_tensors is None:\n",
    "        return all_input_ids\n",
    "\n",
    "    # 否则返回 tensor 并自动 padding\n",
    "    if return_tensors == \"pt\":\n",
    "        padded = tokenizer.pad(\n",
    "            {\"input_ids\": all_input_ids},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return padded\n",
    "\n",
    "    raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(question: str, conv_mode: str = \"llava_v1\") -> str:\n",
    "    \"\"\"构造单样本 prompt\"\"\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "\n",
    "def build_multimodal_batch_inputs_new(\n",
    "    batch, tokenizer, image_processor, device, dtype=torch.float16, conv_mode=\"llava_v1\", max_workers=8\n",
    "):\n",
    "    \"\"\"\n",
    "    批量构建多模态输入 (文本 + 图像)\n",
    "    - 文本部分批量 tokenizer padding\n",
    "    - 图像部分多线程 image_processor 预处理\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 文本批量生成 prompt\n",
    "    prompts = []\n",
    "    for s in batch:\n",
    "        q = s[\"question\"]\n",
    "        conv = conv_templates[conv_mode].copy()\n",
    "        conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + q)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompts.append(conv.get_prompt())\n",
    "\n",
    "    # 2 Tokenizer 批量编码（自动 padding）\n",
    "\n",
    "\n",
    "    tok = tokenizer_image_token(\n",
    "        prompts,\n",
    "        tokenizer,\n",
    "        IMAGE_TOKEN_INDEX,\n",
    "        return_tensors=\"pt\",\n",
    "        padding_side=tokenizer.padding_side # 或 \"left\"\n",
    "    )\n",
    "    # print(tok[\"input_ids\"])\n",
    "    input_ids = tok[\"input_ids\"]\n",
    "    attention_mask = tok[\"attention_mask\"]\n",
    "\n",
    "    # image = batch[0][\"image\"]\n",
    "    # image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device, dtype)\n",
    "    # print(image_tensor)\n",
    "\n",
    "    # 3 图像并行预处理\n",
    "    def _process_img(s):\n",
    "        image = s.get(\"image\", None)\n",
    "        if image is None:\n",
    "            return None\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        try:\n",
    "            return image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device, dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] 图像处理失败: {e}\")\n",
    "            return None\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=min(max_workers, len(batch))) as ex:\n",
    "        image_tensors = list(ex.map(_process_img, batch))   \n",
    "\n",
    "    image_sizes = [s[\"image\"].size for s in batch]\n",
    "    # print(input_ids, image_tensors)\n",
    "    return input_ids, attention_mask, torch.cat(image_tensors, dim=0), image_sizes\n",
    "\n",
    "\n",
    "\n",
    "counters = {k: Counter() for k in [\"llava_match\", \"lmms_match\", \"loose_match\"]}\n",
    "correct_loose, total = 0, 0\n",
    "results = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Evaluating GQA\"):\n",
    "    # ============================================================\n",
    "    # 构建批量输入（文本+图像）\n",
    "    # ============================================================\n",
    "\n",
    "\n",
    "    input_ids_batch, attention_mask_batch, image_tensors, image_sizes = build_multimodal_batch_inputs_new(\n",
    "        batch, tokenizer, image_processor, args.device, torch.float16, args.conv_mode\n",
    "    )\n",
    "\n",
    "    print(f\"input_ids_batch: {input_ids_batch}\")\n",
    "    # print(f\"attention_mask_batch: {attention_mask_batch}\")\n",
    "\n",
    "    # Ground truth 与问题\n",
    "    gts = [s[\"answer\"].strip().lower() for s in batch if s.get(\"answer\")]\n",
    "    questions = [s[\"question\"] for s in batch]\n",
    "\n",
    "    # ============================================================\n",
    "    # 模型生成\n",
    "    # ============================================================\n",
    "\n",
    "    outputs = model_to_use.generate(\n",
    "        inputs=input_ids_batch.to(args.device),\n",
    "        images=image_tensors.to(args.device),\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        attention_mask=attention_mask_batch.to(args.device),\n",
    "        image_sizes=image_sizes,\n",
    "        temperature=0,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    preds = [p.strip().lower() for p in preds]\n",
    "\n",
    "    # ============================================================\n",
    "    # 批量匹配 + 向量化统计\n",
    "    # ============================================================\n",
    "    batch_matches = compute_match_batch(preds, gts)\n",
    "\n",
    "    llava_match = np.fromiter((m[\"llava_match\"] for m in batch_matches), dtype=bool)\n",
    "    lmms_match = np.fromiter((m[\"lmms_match\"] for m in batch_matches), dtype=bool)\n",
    "    loose_match = np.fromiter((m[\"loose_match\"] for m in batch_matches), dtype=bool)\n",
    "\n",
    "    is_correct = lmms_match | loose_match\n",
    "\n",
    "    # 向量化统计更新\n",
    "    counters[\"llava_match\"].update({True: llava_match.sum(), False: len(llava_match) - llava_match.sum()})\n",
    "    counters[\"lmms_match\"].update({True: lmms_match.sum(), False: len(lmms_match) - lmms_match.sum()})\n",
    "    counters[\"loose_match\"].update({True: loose_match.sum(), False: len(loose_match) - loose_match.sum()})\n",
    "\n",
    "    correct_loose += int(is_correct.sum())\n",
    "    total += len(is_correct)\n",
    "    # print(questions, gts)\n",
    "\n",
    "    def single_inference(image, idx):\n",
    "\n",
    "        single_prompt = build_prompt(questions[idx], args.conv_mode)\n",
    "        tok = tokenizer_image_token([single_prompt], tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "        single_input_ids = tok[\"input_ids\"].to(args.device)          # shape (1, L)\n",
    "        single_attention_mask = tok[\"attention_mask\"].to(args.device)\n",
    "\n",
    "        batch_pred = preds[idx]\n",
    "        single_output = model_to_use.generate(\n",
    "            inputs=single_input_ids,\n",
    "            images=image.unsqueeze(0),\n",
    "            image_sizes=[image.size],\n",
    "            attention_mask=single_attention_mask,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            temperature=0,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # print(f\"single_prompt: {single_prompt}\")\n",
    "        # # print(f\"batch_prompt: {tokenizer.decode(input_ids_batch[idx], skip_special_tokens=True)}\")\n",
    "\n",
    "        # print(f\"batch_input_ids: {input_ids_batch[idx]}\")\n",
    "        # print(f\"single_input_ids: {single_input_ids}\")\n",
    "\n",
    "        # print(f\"batch_output: {outputs[idx]}\")\n",
    "        # print(f\"single_output: {single_output}\")\n",
    "\n",
    "        # print(f\"batch_decode: {tokenizer.batch_decode(outputs, skip_special_tokens=True)[idx]}\")\n",
    "        # print(f\"single_decode: {tokenizer.decode(single_output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "        single_pred = tokenizer.decode(single_output[0], skip_special_tokens=True).strip().lower()\n",
    "        if batch_pred != single_pred:\n",
    "\n",
    "            print(\"\\n🔍 Debug: Batch vs Single\")\n",
    "            print(f\"Question: {questions[idx]}\")\n",
    "            print(f\"GT:       {gts[idx]}\")\n",
    "            print(f\"Batch:    '{batch_pred}'\")\n",
    "            print(f\"Single:   '{single_pred}'\")\n",
    "            print(f\"Match?    {batch_pred == single_pred}\")\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        single_inference(image_tensors[i], i)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2efdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutputWithPast"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3181a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "match1_testdev_GQA_100samples_bs2_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs2/pred_gt.json\"\n",
    "match1_testdev_GQA_100samples_bs2_data = read_json(match1_testdev_GQA_100samples_bs2_path)\n",
    "\n",
    "testdev_GQA_100samples_bs2_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs2/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs2_data = read_json(testdev_GQA_100samples_bs2_path)\n",
    "\n",
    "testdev_GQA_100samples_bs2_list_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs2_list/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs2_list_data = read_json(testdev_GQA_100samples_bs2_list_path)\n",
    "\n",
    "def get_data(data):\n",
    "    result = [  (d[\"question\"], d[\"pred\"], d[\"gt\"]) for d in data]\n",
    "    return result\n",
    "\n",
    "match1, b2, b2l = get_data(match1_testdev_GQA_100samples_bs2_data), get_data(testdev_GQA_100samples_bs2_data), get_data(testdev_GQA_100samples_bs2_list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0ab555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is it overcast?', '', 'no', False),\n",
       " ('Who is wearing the dress?',\n",
       "  'the image shows a woman wearing a dress.',\n",
       "  'women',\n",
       "  False),\n",
       " ('Does the utensil on top of the table look clean and black?',\n",
       "  'yes, the utensil on top of the table looks clean and black.',\n",
       "  'no',\n",
       "  False)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match1_bs2_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs2/pred_gt.json\"\n",
    "match1_bs8_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs8/pred_gt.json\"\n",
    "\n",
    "def get_data_with_correct(data):\n",
    "    result = [  (d[\"question\"], d[\"pred\"], d[\"gt\"], d[\"is_correct\"]) for d in data]\n",
    "    return result\n",
    "\n",
    "mbs2, mbs8 = get_data_with_correct(read_json(match1_bs2_path)), get_data_with_correct(read_json(match1_bs8_path))\n",
    "mbs2[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c758585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question: Is the ground blue or brown?, gt: brown, match1: ('the ground in the image is brown.',), match2: \n",
      " question: Is the jacket long sleeved and black?, gt: yes, match1: ('yes, the jacket is long sleeved and black.',), match2: \n",
      " question: Are there beds next to the small outlet?, gt: yes, match1: ('yes, there are beds next to the small outlet.',), match2: \n",
      " question: Are the plates on top of an ottoman?, gt: no, match1: ('no, the plates are not on top of an ottoman. they are on top of a table.',), match2: \n",
      " question: Which side are the white houses on?, gt: left, match1: ('left',), match2: \n",
      " question: Are there either any small refrigerators or microwaves in the picture?, gt: no, match1: ('no, there are no small refrigerators or microwaves in the picture. the image shows a large refrigerator and a microwave.',), match2: yes, there is a small refrigerator and a small microwave in the picture.\n",
      " question: How does that car look like, orange or maybe white?, gt: white, match1: ('the car in the image is white.',), match2: \n",
      " question: What are the drapes around of?, gt: window, match1: ('the drapes are around a window.',), match2: \n",
      " question: On which side is the picture?, gt: left, match1: ('the picture is on the left side of the image.',), match2: \n",
      " question: Is this helicopter on or off?, gt: off, match1: ('the helicopter in the image is off.',), match2: ,\n",
      " question: Do you see any cats?, gt: yes, match1: ('yes, there is a cat in the image.',), match2: \n",
      " question: Is that shoe behind a dog?, gt: no, match1: ('',), match2: no\n",
      " question: Is the field soft and snowy?, gt: no, match1: ('yes, the field is soft and snowy.',), match2: \n",
      " question: Which color is the shirt?, gt: white, match1: ('the shirt is white.',), match2: \n",
      " question: Is there a pear beneath the appliance that looks silver and black?, gt: no, match1: ('no, there is no pear beneath the silver and black appliance. the image shows a silver and black appliance sitting on top of a table, but there is no pear or any other object beneath it.',), match2: \n",
      " question: On which side is the router?, gt: left, match1: ('the router is on the left side of the image.',), match2: router\n",
      " question: What's the man doing?, gt: standing, match1: ('the man in the image is standing in front of a building, possibly a church, and appears to be looking up at the sky. it is not possible to determine his exact actions or intentions from the image alone.',), match2: \n",
      " question: Is the van in front of a balloon?, gt: no, match1: ('no, the van is not in front of a balloon. the image shows a van driving down a street, but there is no indication of a balloon being present.',), match2: yes, the van is in front of a balloon.\n",
      " question: Are the boxes to the right of the man full and square?, gt: yes, match1: ('yes, the boxes to the right of the man are full and square.',), match2: (1) no, the boxes to the right of the man are empty and square.\n",
      " question: Does the blanket look soft and white?, gt: yes, match1: ('yes, the blanket looks soft and white.',), match2: \n",
      " question: How wide is the parking lot made of cement?, gt: wide, match1: ('the parking lot made of cement is quite wide, as it can accommodate multiple vehicles parked side by side.',), match2: \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mbs2)):\n",
    "    a, b = mbs2[i], mbs8[i]\n",
    "    assert(a[0] == b[0] and a[2] == b[2])\n",
    "    if a[3] != b[3]:\n",
    "        print(f' question: {a[0]}, gt: {a[2]}, match1: {a[1], }, match2: {b[1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18c02d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is it overcast?', '', 'no'),\n",
       " ('Who is wearing the dress?',\n",
       "  'the image shows a woman wearing a dress.',\n",
       "  'women'),\n",
       " ('Does the utensil on top of the table look clean and black?',\n",
       "  'yes, the utensil on top of the table looks clean and black.',\n",
       "  'no'),\n",
       " ('Is the surfer that looks wet wearing a wetsuit?', '(no)', 'yes'),\n",
       " ('How tall is the chair in the bottom of the photo?',\n",
       "  'the chair in the bottom of the photo is very tall.',\n",
       "  'short')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ba5559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match1_testdev_GQA_100samples_bs8_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/match1_testdev_GQA_100samples_bs8/pred_gt.json\"\n",
    "match1_testdev_GQA_100samples_bs8_data = read_json(match1_testdev_GQA_100samples_bs8_path)\n",
    "\n",
    "testdev_GQA_100samples_bs8_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs8/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs8_data = read_json(testdev_GQA_100samples_bs8_path)\n",
    "\n",
    "testdev_GQA_100samples_bs8_list_path = r\"/home/czj/llava15_test/LLaVA/eval_GQA_results/testdev_GQA_100samples_bs8_list/pred_gt.json\"\n",
    "testdev_GQA_100samples_bs8_list_data = read_json(testdev_GQA_100samples_bs8_list_path)\n",
    "\n",
    "match2, b8, b8l = get_data(match1_testdev_GQA_100samples_bs8_data), get_data(testdev_GQA_100samples_bs8_data), get_data(testdev_GQA_100samples_bs8_list_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f68a73d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match2 == b8, match2 == b8l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a93ef704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is it overcast?', '', 'no'),\n",
       " ('Who is wearing the dress?', '', 'women'),\n",
       " ('Does the utensil on top of the table look clean and black?',\n",
       "  'yes, the utensil on top of the table looks clean and black.',\n",
       "  'no'),\n",
       " ('Is the surfer that looks wet wearing a wetsuit?', '(no)', 'yes'),\n",
       " ('How tall is the chair in the bottom of the photo?', '', 'short')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70050a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question: Who is wearing the dress?, gt: women, match1: ('the image shows a woman wearing a dress.',), match2: \n",
      " question: How tall is the chair in the bottom of the photo?, gt: short, match1: ('the chair in the bottom of the photo is very tall.',), match2: \n",
      " question: What kind of device is on top of the desk?, gt: keyboard, match1: ('there is a laptop computer on top of the desk.',), match2: \n",
      " question: What is the airplane flying above?, gt: ocean, match1: ('the airplane is flying above a city.',), match2: \n",
      " question: What color are the pants?, gt: red, match1: ('<image>',), match2: \n",
      " question: Is the ground blue or brown?, gt: brown, match1: ('the ground in the image is brown.',), match2: \n",
      " question: What is around the open window?, gt: drapes, match1: ('there is a chair placed next to the open window.',), match2: \n",
      " question: What's around the window?, gt: drapes, match1: ('there is a white frame around the window.',), match2: \n",
      " question: Who is standing at the table?, gt: woman, match1: ('a man is standing at the table.',), match2: \n",
      " question: Are there drapes to the right of the bed?, gt: yes, match1: ('no, there are no drapes to the right of the bed.',), match2: \n",
      " question: What is hanging above the chalkboard?, gt: picture, match1: ('is a chalkboard.',), match2: a chalkboard\n",
      " question: What device is sitting next to the mouse pad?, gt: keyboard, match1: ('a computer mouse is sitting next to the mouse pad.',), match2: \n",
      " question: Does the sweater look open and blue?, gt: yes, match1: (\"'\",), match2: \n",
      " question: Is the jacket long sleeved and black?, gt: yes, match1: ('yes, the jacket is long sleeved and black.',), match2: \n",
      " question: Are there beds next to the small outlet?, gt: yes, match1: ('yes, there are beds next to the small outlet.',), match2: \n",
      " question: On which side of the picture is the leather bag?, gt: right, match1: ('the leather bag is on the left side of the picture.',), match2: \n",
      " question: Is the blue pillow square and large?, gt: no, match1: ('ure, the blue pillow is square and large.',), match2: \n",
      " question: Are the plates on top of an ottoman?, gt: no, match1: ('no, the plates are not on top of an ottoman. they are on top of a table.',), match2: \n",
      " question: Is the fence made of cement or aluminum?, gt: aluminum, match1: ('the fence in the image is made of cement.',), match2: \n",
      " question: Which side are the white houses on?, gt: left, match1: ('left',), match2: \n",
      " question: Are there either any small refrigerators or microwaves in the picture?, gt: no, match1: ('no, there are no small refrigerators or microwaves in the picture. the image shows a large refrigerator and a microwave.',), match2: yes, there is a small refrigerator and a small microwave in the picture.\n",
      " question: How does that car look like, orange or maybe white?, gt: white, match1: ('the car in the image is white.',), match2: \n",
      " question: Is the bag made of leather lying on top of a sofa?, gt: no, match1: ('no, the bag made of leather is not lying on top of a sofa. it is placed on the ground.',), match2: no\n",
      " question: What are the drapes around of?, gt: window, match1: ('the drapes are around a window.',), match2: \n",
      " question: On which side is the picture?, gt: left, match1: ('the picture is on the left side of the image.',), match2: \n",
      " question: How is the clothing item that is pink called?, gt: tank top, match1: ('the clothing item that is pink is a dress.',), match2: \n",
      " question: Which kind of clothing is not pink?, gt: hat, match1: ('is not pink',), match2: shirt\n",
      " question: Is this helicopter on or off?, gt: off, match1: ('the helicopter in the image is off.',), match2: ,\n",
      " question: Do you see any cats?, gt: yes, match1: ('yes, there is a cat in the image.',), match2: \n",
      " question: Is that shoe behind a dog?, gt: no, match1: ('',), match2: no\n",
      " question: What kind of clothing is sleeveless?, gt: tank top, match1: ('the image shows a person wearing a sleeveless shirt.',), match2: leeveless shirt\n",
      " question: Is the field soft and snowy?, gt: no, match1: ('yes, the field is soft and snowy.',), match2: \n",
      " question: What fruits are beneath the microwave?, gt: bananas, match1: ('there are no fruits beneath the microwave in the image. the image shows a microwave sitting on top of a counter or table, with nothing else beneath it.',), match2: \n",
      " question: Which color is the shirt?, gt: white, match1: ('the shirt is white.',), match2: \n",
      " question: What is beneath the microwave?, gt: bananas, match1: ('beneath the microwave, there is a countertop.',), match2: \n",
      " question: Is there a pear beneath the appliance that looks silver and black?, gt: no, match1: ('no, there is no pear beneath the silver and black appliance. the image shows a silver and black appliance sitting on top of a table, but there is no pear or any other object beneath it.',), match2: \n",
      " question: Is this a bed or a cabinet?, gt: cabinet, match1: ('this is a bed.',), match2: orry, i'm not sure what you're asking. could you please provide more context or clarify your question?\n",
      " question: On which side is the router?, gt: left, match1: ('the router is on the left side of the image.',), match2: router\n",
      " question: What is the color of the pants?, gt: gray, match1: ('the color of the pants is black.',), match2: \n",
      " question: Who is wearing a shirt?, gt: girl, match1: ('a person is wearing a shirt.',), match2: \n",
      " question: What's the man doing?, gt: standing, match1: ('the man in the image is standing in front of a building, possibly a church, and appears to be looking up at the sky. it is not possible to determine his exact actions or intentions from the image alone.',), match2: \n",
      " question: Which kind of vehicle is in front of the flag?, gt: van, match1: ('a car is in front of the flag.',), match2: <image>\n",
      " question: What is the vehicle that is in front of the flag?, gt: van, match1: ('the vehicle in front of the flag is a truck.',), match2: </image>\n",
      " question: Is the van in front of a balloon?, gt: no, match1: ('no, the van is not in front of a balloon. the image shows a van driving down a street, but there is no indication of a balloon being present.',), match2: yes, the van is in front of a balloon.\n",
      " question: How is the item of furniture that is plaid called?, gt: bed, match1: ('the item of furniture that is plaid is called a sofa or a couch. both terms are commonly used to describe a long, narrow seat with a back and armrests, typically used for seating multiple people. the pattern of the fabric on the sofa or couch is called plaid,',), match2: (1) the item of furniture that is plaid is called a couch.\n",
      "(2) the furniture that is plaid is a couch.\n",
      "(3) the couch is plaid in color.\n",
      "(4) the couch is a piece of furniture.\n",
      "(5\n",
      " question: Are the boxes to the right of the man full and square?, gt: yes, match1: ('yes, the boxes to the right of the man are full and square.',), match2: (1) no, the boxes to the right of the man are empty and square.\n",
      " question: Is the horse next to the other horse both baby and brown?, gt: no, match1: ('yes, the horse next to the other horse is both a baby and brown.',), match2: (1) the horse next to the other horse is both baby and brown.\n",
      "\n",
      "(2) the horse next to the other horse is a baby brown horse.\n",
      "\n",
      "(3) the horse next to the other horse is a brown baby horse.\n",
      "\n",
      "(4) the horse next to the other horse\n",
      " question: What appliance is the refrigerator larger than?, gt: stove, match1: ('the refrigerator is larger than the oven.',), match2: <image>\n",
      " question: What is the color of the glove?, gt: black, match1: ('the color of the glove is white.',), match2: \n",
      " question: Does the blanket look soft and white?, gt: yes, match1: ('yes, the blanket looks soft and white.',), match2: \n",
      " question: Are there refrigerators to the left of the stove?, gt: yes, match1: ('no, there are no refrigerators to the left of the stove.',), match2: <image>\n",
      " question: Which type of clothing is pink?, gt: gown, match1: ('is wearing a pink shirt.',), match2: wear\n",
      " question: What is the person that is sitting down sitting atop?, gt: stairs, match1: ('the person sitting down is sitting atop a pile of books.',), match2: \n",
      " question: What items of furniture are to the left of the boy?, gt: tables, match1: ('there are two chairs to the left of the boy.',), match2: (1) a bookshelf and (2) a desk are to the left of the boy.\n",
      " question: What is in front of the wall that is not short?, gt: shelf, match1: ('there is a chair in front of the wall that is not short.',), match2: \n",
      " question: How wide is the parking lot made of cement?, gt: wide, match1: ('the parking lot made of cement is quite wide, as it can accommodate multiple vehicles parked side by side.',), match2: \n",
      " question: What is the device in front of the flat computer?, gt: phone, match1: ('there is a flat computer monitor in front of the device.',), match2: the device in front of the flat computer is a tablet.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(match2)):\n",
    "    a, b = match1[i], match2[i]\n",
    "    assert(a[0] == b[0] and a[2] == b[2])\n",
    "    if a != b:\n",
    "        print(f' question: {a[0]}, gt: {a[2]}, match1: {a[1], }, match2: {b[1]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73264c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292dcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 从缓存加载数据样本...\n",
      "缓存样本数: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'imageId', 'question', 'answer', 'fullAnswer', 'types', 'groups', 'semantic', 'structural', 'image_data', 'image'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "CACHE_FILE = \"gqa_cache.pkl\"\n",
    "if Path(CACHE_FILE).exists():\n",
    "    print(\"✅ 从缓存加载数据样本...\")\n",
    "    with open(CACHE_FILE, \"rb\") as f:\n",
    "        dataset_cache = pickle.load(f)\n",
    "else:\n",
    "    print(\"🚀 第一次加载数据集...\")\n",
    "    gqa_loader = GQALoader(gqa_root=\"/home/Dataset/Dataset/GQA\")\n",
    "    dataset = gqa_loader.load_dataset(split=\"train_balanced\", num_samples=10)\n",
    "    dataset_cache = [gqa_loader.process_sample(dataset[i]) for i in range(min(10, len(dataset)))]\n",
    "\n",
    "    \n",
    "    # 存缓存\n",
    "    with open(CACHE_FILE, \"wb\") as f:\n",
    "        pickle.dump(dataset_cache, f)\n",
    "    print(\"✅ 已缓存前10个样本到 gqa_cache.pkl\")\n",
    "\n",
    "print(f\"缓存样本数: {len(dataset_cache)}\")\n",
    "sample = dataset_cache[0]\n",
    "sample.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8218a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama_masked. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型加载完成\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/czj/llava15_test/llava-v1.5-7b\"\n",
    "clip_path = \"/home/czj/llava15_test/clip-vit-large-patch14-336\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    device_map=\"cuda:1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_masked_model=True\n",
    ")\n",
    "\n",
    "print(\"✅ 模型加载完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "858aacc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llava.model.language_model.llava_llama_masked.LlavaLlamaForCausalLMMasked"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db2fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 原图尺寸: (500, 270), tensor形状: torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset_cache[0]\n",
    "image = sample[\"image\"].convert(\"RGB\")\n",
    "\n",
    "image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(model.device, dtype=torch.float16)\n",
    "\n",
    "print(f\"[DEBUG] 原图尺寸: {image.size}, tensor形状: {image_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4357ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题： Is the sky dark?  模型输出: Yes, the sky is dark in the image.\n",
      "问题： What is on the white wall?  模型输出: There is a painting on the white wall.\n",
      "问题： Is that pipe red?  模型输出: No, the pipe is not red. It is a black pipe.\n",
      "问题： Is the tall clock small or large?  模型输出: The tall clock is large, as it is described as a large clock tower.\n",
      "问题： Who is wearing a shirt?  模型输出: A boy is wearing a shirt.\n",
      "问题： What do you think is he sleeping in?  模型输出: The man is sleeping in a bed, which is covered with a white comforter.\n",
      "问题： Is the cheese to the left of the food on the plate?  模型输出: Yes, the cheese is to the left of the food on the plate.\n",
      "问题： What is the piece of furniture that he is sleeping in?  模型输出: The man is sleeping in a bed.\n",
      "问题： What kind of furniture is to the right of the chair?  模型输出: There is a couch to the right of the chair.\n",
      "问题： Is the steel spatula in the top of the image?  模型输出: Yes, the steel spatula is in the top of the image.\n"
     ]
    }
   ],
   "source": [
    "from llava.conversation import conv_templates\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "for sample in dataset_cache:\n",
    "    image = sample[\"image\"].convert(\"RGB\")\n",
    "\n",
    "    image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(model.device, dtype=torch.float16)\n",
    "    question = sample[\"question\"]\n",
    "\n",
    "    conv = conv_templates[\"llava_v1\"].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            inputs=input_ids,\n",
    "            images=[image_tensor],\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            temperature=0,\n",
    "            top_p=None,\n",
    "            num_beams=1\n",
    "        )\n",
    "\n",
    "\n",
    "    # answer = tokenizer.batch_decode(output_tokens[0], skip_special_tokens=True)\n",
    "    answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(\"问题：\", question, \" 模型输出:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90f37212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mask_visual_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41f6ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czj/anaconda3/envs/llava15/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object in the image is red.\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# # 载入模型\n",
    "# tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "#     model_path=model_path,\n",
    "#     model_base=None,\n",
    "#     model_name=get_model_name_from_path(model_path),\n",
    "#     device_map=\"cuda:1\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     # use_masked_model=True\n",
    "# )\n",
    "# 图像\n",
    "image = Image.open(\"/home/czj/llava15_test/LLaVA/images/llava_v1_5_radar.jpg\").convert(\"RGB\")\n",
    "image_tensor = process_images([image], image_processor, model.config).to(model.device, dtype=torch.float16)\n",
    "\n",
    "# prompt\n",
    "question = \"What is the color of the object in the image?\"\n",
    "conv = conv_templates[\"llava_v1\"].copy()\n",
    "conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "\n",
    "# 生成\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensor,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5e697a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   450,  1203,   297,   278,  1967,   338,  2654, 29889,     2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2faf76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Last hidden shape: torch.Size([1, 623, 4096])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        images=image_tensor,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "print(f\"[DEBUG] Last hidden shape: {outputs.hidden_states[-1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "291233f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 模型输出: <s>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        inputs=input_ids,\n",
    "        images=image_tensor,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        top_p=None,\n",
    "        num_beams=1\n",
    "    )\n",
    "\n",
    "answer = tokenizer.batch_decode(output_tokens[0], skip_special_tokens=True)[0]\n",
    "print(\"🧩 模型输出:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
